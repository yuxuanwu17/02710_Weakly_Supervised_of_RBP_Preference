{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1_ucA9cBr9OW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bac1a8d-40aa-4f7f-f2e0-2ec35ad378db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: www-browser: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: links2: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: elinks: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: links: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: lynx: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: w3m: not found\n",
            "xdg-open: no method available for opening 'https://accounts.google.com/o/oauth2/auth?client_id=564921029129.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fgd-ocaml-auth.appspot.com%2Foauth2callback&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force&state=dAHOSyPaCBSaWvhETqJXq1nr2wgBoqeDPjhtr94Jw1M'\n",
            "/bin/sh: 1: firefox: not found\n",
            "/bin/sh: 1: google-chrome: not found\n",
            "/bin/sh: 1: chromium-browser: not found\n",
            "/bin/sh: 1: open: not found\n",
            "Cannot retrieve auth tokens.\n",
            "Failure(\"Error opening URL:https://accounts.google.com/o/oauth2/auth?client_id=564921029129.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fgd-ocaml-auth.appspot.com%2Foauth2callback&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force&state=dAHOSyPaCBSaWvhETqJXq1nr2wgBoqeDPjhtr94Jw1M\")\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libgpm2:amd64.\n",
            "(Reading database ... 155208 files and directories currently installed.)\n",
            "Preparing to unpack .../libgpm2_1.20.7-5_amd64.deb ...\n",
            "Unpacking libgpm2:amd64 (1.20.7-5) ...\n",
            "Selecting previously unselected package w3m.\n",
            "Preparing to unpack .../w3m_0.5.3-36build1_amd64.deb ...\n",
            "Unpacking w3m (0.5.3-36build1) ...\n",
            "Setting up libgpm2:amd64 (1.20.7-5) ...\n",
            "Setting up w3m (0.5.3-36build1) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "/content\n",
            "/content/drive\n",
            "/content\n",
            "/\n",
            "Access token retrieved correctly.\n"
          ]
        }
      ],
      "source": [
        "!sudo add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!sudo apt-get update -qq 2>&1 > /dev/null\n",
        "!sudo apt -y install -qq google-drive-ocamlfuse 2>&1 > /dev/null\n",
        "!google-drive-ocamlfuse\n",
        "\n",
        "!sudo apt-get install -qq w3m # to act as web browser \n",
        "!xdg-settings set default-web-browser w3m.desktop # to set default browser\n",
        "%cd /content\n",
        "!mkdir drive\n",
        "%cd drive\n",
        "!mkdir MyDrive\n",
        "%cd ..\n",
        "%cd ..\n",
        "!google-drive-ocamlfuse /content/drive/MyDrive"
      ],
      "id": "1_ucA9cBr9OW"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKI_ULaqsB_t",
        "outputId": "99172f43-e9aa-458b-d213-fe3cb584783e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri May  6 17:48:39 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ],
      "id": "nKI_ULaqsB_t"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "722178ed-e1c5-4348-ac97-144cf339b538",
        "outputId": "af8549ad-e33d-4fde-e86b-029fb8f99c62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.16-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 654 kB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 56.9 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.11-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 65.1 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=032f279115247758d59a568e4ea41d673f234d1888569fc27fd2ca65c3369068\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.11 setproctitle-1.2.3 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.16\n",
            "Collecting torchsummaryX\n",
            "  Downloading torchsummaryX-1.3.0-py3-none-any.whl (3.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.3.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.11.0+cu113)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torchsummaryX) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torchsummaryX) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torchsummaryX) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchsummaryX) (4.2.0)\n",
            "Installing collected packages: torchsummaryX\n",
            "Successfully installed torchsummaryX-1.3.0\n",
            "Collecting pytorch_warmup\n",
            "  Downloading pytorch-warmup-0.1.0.tar.gz (314 kB)\n",
            "\u001b[K     |████████████████████████████████| 314 kB 652 kB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_warmup) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1->pytorch_warmup) (4.2.0)\n",
            "Building wheels for collected packages: pytorch-warmup\n",
            "  Building wheel for pytorch-warmup (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-warmup: filename=pytorch_warmup-0.1.0-py3-none-any.whl size=5802 sha256=31c35a53cc70007f48a65b936d656a0267abbc98dbbfede4d23404bf0856309b\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/5d/24/9475e442daa1e9332c122c79fb5131b9e4e91946009365902e\n",
            "Successfully built pytorch-warmup\n",
            "Installing collected packages: pytorch-warmup\n",
            "Successfully installed pytorch-warmup-0.1.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.utils as utils\n",
        "import gzip\n",
        "!pip install wandb\n",
        "import os\n",
        "import wandb\n",
        "! pip install torchsummaryX\n",
        "from torchsummaryX import summary\n",
        "!pip install pytorch_warmup\n",
        "import pytorch_warmup as warmup\n",
        "def create_folder(folder_path):\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.mkdir(folder_path)\n",
        "\n",
        "checkpoints_store_path = \"/content/drive/MyDrive/02710_proj/store_checkpoints\"\n",
        "create_folder(checkpoints_store_path)\n",
        "\n",
        "batch_size = 1\n",
        "epochs = 20\n",
        "lr = 1e-3\n",
        "\n",
        "model_id = f\"WSCNN\"\n",
        "data_path = checkpoints_store_path + model_id\n",
        "create_folder(data_path)\n",
        "instance_length = 40\n",
        "instance_stride = 5\n",
        "# wandb.init(project=\"02710\", entity=\"saltedfish\",name=model_id)\n",
        "# wandb.config = {\n",
        "#   \"learning_rate\": lr,\n",
        "#   \"epochs\": epochs,\n",
        "#   \"batch_size\": batch_size,\n",
        "# }\n"
      ],
      "id": "722178ed-e1c5-4348-ac97-144cf339b538"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "68abed00-2d14-40d3-bc0d-c5d304648970",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7effd8b4-60d0-4726-c790-5c09bbc3dab1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'> chr1,+,866159,866259; class:0\\n'\n",
            "b'GCCTCGGTCCTGTTCTTCCTGATGCGTGTCTGCTGAGGCCAGGAGCTGGCTTTGGCCCAT\\n'\n",
            "b'GGGCCTGTCCTAGTGGGAGGCCCCAGCATGTTGAGCCAGTA\\n'\n",
            "b'> chr1,+,870491,870591; class:0\\n'\n",
            "b'GGATTGGGCTGAATTAGCAAGAAGAGGAGAAATGAGGGAAGAAAAGAGTTAAATGCATGT\\n'\n",
            "b'TGATTCCAAGCCCCCGCCTGCCGGGGGGACAGCGGGAGGTT\\n'\n",
            "b'> chr1,+,949147,949247; class:0\\n'\n",
            "b'AGGTAAAGGGAGGCCACGGGATGGCGGTGGGCAGCTGGCCTTCTAGTAACGAGCCCTCAG\\n'\n",
            "b'TGCCTTCTGTGCCTGGGGTCCCTGCCGGCGGGATGTAGAGG\\n'\n",
            "b'> chr1,+,949360,949460; class:0\\n'\n"
          ]
        }
      ],
      "source": [
        "a_file = gzip.open(\"/content/drive/MyDrive/iDeepS-master/datasets/clip/10_PARCLIP_ELAVL1A_hg19/30000/training_sample_0/sequences.fa.gz\", \"rb\")\n",
        "contents = a_file.readlines()\n",
        "\n",
        "for i in range(10):\n",
        "    print(contents[i])"
      ],
      "id": "68abed00-2d14-40d3-bc0d-c5d304648970"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample preparation"
      ],
      "metadata": {
        "id": "D7HzsEljT6Kt"
      },
      "id": "D7HzsEljT6Kt"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7e43435b-adb0-4177-9482-accc9759e3ae"
      },
      "outputs": [],
      "source": [
        "def read_file_helper(file_path):\n",
        "    res = []\n",
        "    with gzip.open(file_path, \"rb\") as f:\n",
        "        for line in f.readlines():\n",
        "            line = str(line)[2:-3]\n",
        "            \n",
        "            if line.startswith(\">\"):\n",
        "                indicator = 0\n",
        "                tmp = []\n",
        "                loc, y = line.strip().split(\";\")\n",
        "                chr_num, sign, start, end = loc[2:].split(\",\")\n",
        "                tmp.extend([chr_num, sign, int(start), int(end), y[-1]])\n",
        "            \n",
        "            else:\n",
        "                indicator += 1\n",
        "                tmp.append(line)\n",
        "                if indicator == 2:\n",
        "                    res.append(tmp)\n",
        "    df = pd.DataFrame(res, columns = [\"chr_num\", \"sign\", \"start\", \"end\", \"y\", \"seq_part1\", \"seq_part2\"])\n",
        "    df[\"seq\"] = df[\"seq_part1\"] + df[\"seq_part2\"]\n",
        "    df[\"y\"] = df[\"y\"].astype(int)\n",
        "    return df\n",
        "                \n",
        "\n",
        "def embed(sequence, instance_len, instance_stride):\n",
        "    instance_num = int((len(sequence) - instance_len) / instance_stride) + 1\n",
        "    bag = []\n",
        "    for i in range(instance_num):\n",
        "        instance = sequence[i * instance_stride:i * instance_stride + instance_len]\n",
        "        instance = one_hot_encode(instance)\n",
        "        bag.append(instance)\n",
        "    bag = np.stack(bag).astype(float)\n",
        "    return bag\n",
        "\n",
        "def one_hot_encode(seq):\n",
        "    arrays = [np.array([1, 0, 0, 0]),\n",
        "             np.array([0, 1, 0, 0]),\n",
        "             np.array([0, 0, 1, 0]),\n",
        "             np.array([0, 0, 0, 1]),\n",
        "             np.array([0.25, 0.25, 0.25, 0.25])]\n",
        "             \n",
        "    mapping = dict(zip(\"ACGTN\", arrays))\n",
        "   \n",
        "    return np.vstack([mapping[i] for i in seq])\n",
        "\n",
        "def create_bag(seqs, instance_len=instance_length, instance_stride=instance_stride):\n",
        "    bags = []\n",
        "    for seq in seqs:\n",
        "        # rev_seq = reverse_complement(seq) \n",
        "        # seq = seq + \"N\"*instance_length + rev_seq\n",
        "        bags.append(embed(seq, instance_len, instance_stride)) \n",
        "        \n",
        "    return np.array(bags).astype(float)\n",
        "\n",
        "\n",
        "class LibriSamples(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_path):\n",
        "        df = read_file_helper(data_path)\n",
        "        df[\"seq\"] = df[\"seq_part1\"] + df[\"seq_part2\"]\n",
        "        # print(df[\"seq\"])\n",
        "        # df[\"rev_seq\"] = df[\"seq\"].apply(lambda seq : reverse_complement(seq))\n",
        "        # print(df[\"rev_seq\"])\n",
        "\n",
        "        # df[\"seq\"] = df[\"seq\"] + \"N\"*instance_length + df[\"rev_seq\"]\n",
        "        self.X, self.Y = create_bag(df[\"seq\"]), df[\"y\"].to_numpy()\n",
        "        \n",
        "        assert len(self.X) == len(self.Y)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        y = [0, 1] if self.Y[item] == 1 else [1, 0]\n",
        "        return self.X[item], np.array(y)"
      ],
      "id": "7e43435b-adb0-4177-9482-accc9759e3ae"
    },
    {
      "cell_type": "code",
      "source": [
        "def reverse_complement(seq):\n",
        "    mapping = str.maketrans(\"ATCG\",\"TAGC\")\n",
        "    return seq.translate(mapping)[::-1]\n",
        "\n",
        "# seq = \"AAAAAATTTNNNTCCCC\"\n",
        "# rev_seq = reverse_complement(\"AAAAAATTTNNNTCCCC\")\n",
        "# seq = seq + \"N\"*instance_length + rev_seq\n",
        "# print(seq)"
      ],
      "metadata": {
        "id": "HARwndOYfbUR"
      },
      "id": "HARwndOYfbUR",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "33c1f01f-2acb-463a-aaf7-1fc4cc489a85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4792ea1-9f7d-4bf5-80a1-38d7341c329c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[1., 0., 0., 0.],\n",
            "          [0., 0., 0., 1.],\n",
            "          [0., 1., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 1., 0.],\n",
            "          [0., 1., 0., 0.],\n",
            "          [0., 1., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0., 1.],\n",
            "          [0., 0., 1., 0.],\n",
            "          [0., 1., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0., 1.],\n",
            "          [0., 1., 0., 0.],\n",
            "          [0., 0., 0., 1.]],\n",
            "\n",
            "         [[0., 1., 0., 0.],\n",
            "          [0., 0., 0., 1.],\n",
            "          [0., 0., 0., 1.],\n",
            "          ...,\n",
            "          [0., 0., 0., 1.],\n",
            "          [0., 0., 0., 1.],\n",
            "          [0., 0., 1., 0.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1., 0., 0., 0.],\n",
            "          [0., 1., 0., 0.],\n",
            "          [1., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 1., 0., 0.],\n",
            "          [1., 0., 0., 0.],\n",
            "          [1., 0., 0., 0.]],\n",
            "\n",
            "         [[1., 0., 0., 0.],\n",
            "          [0., 0., 0., 1.],\n",
            "          [0., 1., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 1., 0.],\n",
            "          [0., 0., 1., 0.],\n",
            "          [1., 0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 1., 0.],\n",
            "          [0., 0., 1., 0.],\n",
            "          [1., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0., 1.],\n",
            "          [0., 0., 0., 1.],\n",
            "          [0., 1., 0., 0.]]]], dtype=torch.float64) tensor([[1, 0]])\n"
          ]
        }
      ],
      "source": [
        "train_data_path = \"/content/drive/MyDrive/iDeepS-master/datasets/clip/10_PARCLIP_ELAVL1A_hg19/30000/training_sample_0/sequences.fa.gz\"\n",
        "valid_data_path = \"/content/drive/MyDrive/iDeepS-master/datasets/clip/10_PARCLIP_ELAVL1A_hg19/30000/test_sample_0/sequences.fa.gz\"\n",
        "\n",
        "train_data = LibriSamples(train_data_path)\n",
        "valid_data = LibriSamples(valid_data_path)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size = batch_size, shuffle=True)\n",
        "\n",
        "# y_num_0 = []\n",
        "# y_num_1 = []\n",
        "for x, y in train_loader:\n",
        "    print(x, y)\n",
        "    break\n",
        "    # if y == 0:\n",
        "    #   y_num_0.append(y)\n",
        "    # else:\n",
        "    #   y_num_1.append(y)"
      ],
      "id": "33c1f01f-2acb-463a-aaf7-1fc4cc489a85"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Az8K73pH1svf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56c63aff-b68d-42b7-b26f-97bace85f706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 13, 40, 4])\n"
          ]
        }
      ],
      "source": [
        "print(x.shape)\n",
        "# print(len(y_num_0))\n",
        "# print(len(y_num_1))"
      ],
      "id": "Az8K73pH1svf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WeakRM"
      ],
      "metadata": {
        "id": "Mg5mAuMPAW-Z"
      },
      "id": "Mg5mAuMPAW-Z"
    },
    {
      "cell_type": "code",
      "source": [
        "class WeakRM(nn.Module):\n",
        "    \"\"\"\n",
        "    used for channel = 4, AGCT\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, training=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.inst_conv1 = nn.Sequential(\n",
        "            nn.Conv1d(4, 32, kernel_size=15, padding=7, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "        )\n",
        "\n",
        "        self.inst_conv2 = nn.Sequential(\n",
        "            nn.Conv1d(32, 16, kernel_size=5, padding=2, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "        self.attention_v = nn.Sequential(\n",
        "            nn.Linear(320, 128),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.attention_u = nn.Sequential(\n",
        "            nn.Linear(320, 128),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.attention_weights = nn.Sequential(\n",
        "            nn.Linear(128, 1),\n",
        "        )\n",
        "\n",
        "        self.cls = nn.Sequential(\n",
        "            nn.Linear(320, 2),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "    def forward(self, inputs, training=True, mask=None):\n",
        "        inputs = torch.squeeze(inputs, 0)\n",
        "        inputs = inputs.permute((0, 2, 1))  # torch.Size([13, 5, 40])\n",
        "        inst_conv1 = self.inst_conv1(inputs)  # torch.Size([13, 32, 20])\n",
        "        inst_conv1 = self.dropout(inst_conv1)\n",
        "        inst_conv2 = self.inst_conv2(inst_conv1)\n",
        "\n",
        "        inst_features = nn.Flatten()(inst_conv2)\n",
        "\n",
        "        attention_v = self.attention_v(inst_features)\n",
        "        attention_u = self.attention_v(inst_features)\n",
        "\n",
        "        # print(attention_u*attention_v)\n",
        "        # print(self.attention_weights(attention_u * attention_v))\n",
        "\n",
        "        gated_attention = self.attention_weights(attention_u * attention_v).permute((1, 0))\n",
        "        # print(gated_attention)\n",
        "\n",
        "        gated_attention = self.softmax(gated_attention)  # torch.Size([1, 13])\n",
        "\n",
        "        bag_features = torch.matmul(gated_attention, inst_features)\n",
        "\n",
        "        bag_probability = self.cls(bag_features)\n",
        "\n",
        "        return bag_probability, gated_attention"
      ],
      "metadata": {
        "id": "-yjPOPW193_N"
      },
      "id": "-yjPOPW193_N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand((1, 13, 40, 4))\n",
        "encoder = WeakRM()\n",
        "summary(encoder, x)"
      ],
      "metadata": {
        "id": "9ZqbC6Ll1akE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2754a60a-2dc2-4237-8d9b-4f0c37b038c7"
      },
      "id": "9ZqbC6Ll1akE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================\n",
            "                              Kernel Shape  Output Shape   Params Mult-Adds\n",
            "Layer                                                                      \n",
            "0_inst_conv1.Conv1d_0          [4, 32, 15]  [13, 32, 40]   1.952k     76.8k\n",
            "1_inst_conv1.ReLU_1                      -  [13, 32, 40]        -         -\n",
            "2_inst_conv1.MaxPool1d_2                 -  [13, 32, 20]        -         -\n",
            "3_dropout                                -  [13, 32, 20]        -         -\n",
            "4_inst_conv2.Conv1d_0          [32, 16, 5]  [13, 16, 20]   2.576k     51.2k\n",
            "5_inst_conv2.ReLU_1                      -  [13, 16, 20]        -         -\n",
            "6_attention_v.Linear_0          [320, 128]     [13, 128]  41.088k    40.96k\n",
            "7_attention_v.Tanh_1                     -     [13, 128]        -         -\n",
            "8_attention_v.Linear_0          [320, 128]     [13, 128]        -    40.96k\n",
            "9_attention_v.Tanh_1                     -     [13, 128]        -         -\n",
            "10_attention_weights.Linear_0     [128, 1]       [13, 1]    129.0     128.0\n",
            "11_softmax                               -       [1, 13]        -         -\n",
            "12_cls.Linear_0                   [320, 2]        [1, 2]    642.0     640.0\n",
            "13_cls.Sigmoid_1                         -        [1, 2]        -         -\n",
            "----------------------------------------------------------------------------\n",
            "                        Totals\n",
            "Total params           46.387k\n",
            "Trainable params       46.387k\n",
            "Non-trainable params       0.0\n",
            "Mult-Adds             210.688k\n",
            "============================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  df_sum = df.sum()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                              Kernel Shape  Output Shape   Params  Mult-Adds\n",
              "Layer                                                                       \n",
              "0_inst_conv1.Conv1d_0          [4, 32, 15]  [13, 32, 40]   1952.0    76800.0\n",
              "1_inst_conv1.ReLU_1                      -  [13, 32, 40]      NaN        NaN\n",
              "2_inst_conv1.MaxPool1d_2                 -  [13, 32, 20]      NaN        NaN\n",
              "3_dropout                                -  [13, 32, 20]      NaN        NaN\n",
              "4_inst_conv2.Conv1d_0          [32, 16, 5]  [13, 16, 20]   2576.0    51200.0\n",
              "5_inst_conv2.ReLU_1                      -  [13, 16, 20]      NaN        NaN\n",
              "6_attention_v.Linear_0          [320, 128]     [13, 128]  41088.0    40960.0\n",
              "7_attention_v.Tanh_1                     -     [13, 128]      NaN        NaN\n",
              "8_attention_v.Linear_0          [320, 128]     [13, 128]      NaN    40960.0\n",
              "9_attention_v.Tanh_1                     -     [13, 128]      NaN        NaN\n",
              "10_attention_weights.Linear_0     [128, 1]       [13, 1]    129.0      128.0\n",
              "11_softmax                               -       [1, 13]      NaN        NaN\n",
              "12_cls.Linear_0                   [320, 2]        [1, 2]    642.0      640.0\n",
              "13_cls.Sigmoid_1                         -        [1, 2]      NaN        NaN"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d7a6e332-0e1e-48ab-83bb-e570eb882935\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_inst_conv1.Conv1d_0</th>\n",
              "      <td>[4, 32, 15]</td>\n",
              "      <td>[13, 32, 40]</td>\n",
              "      <td>1952.0</td>\n",
              "      <td>76800.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_inst_conv1.ReLU_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[13, 32, 40]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_inst_conv1.MaxPool1d_2</th>\n",
              "      <td>-</td>\n",
              "      <td>[13, 32, 20]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_dropout</th>\n",
              "      <td>-</td>\n",
              "      <td>[13, 32, 20]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_inst_conv2.Conv1d_0</th>\n",
              "      <td>[32, 16, 5]</td>\n",
              "      <td>[13, 16, 20]</td>\n",
              "      <td>2576.0</td>\n",
              "      <td>51200.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5_inst_conv2.ReLU_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[13, 16, 20]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6_attention_v.Linear_0</th>\n",
              "      <td>[320, 128]</td>\n",
              "      <td>[13, 128]</td>\n",
              "      <td>41088.0</td>\n",
              "      <td>40960.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7_attention_v.Tanh_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[13, 128]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8_attention_v.Linear_0</th>\n",
              "      <td>[320, 128]</td>\n",
              "      <td>[13, 128]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>40960.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9_attention_v.Tanh_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[13, 128]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10_attention_weights.Linear_0</th>\n",
              "      <td>[128, 1]</td>\n",
              "      <td>[13, 1]</td>\n",
              "      <td>129.0</td>\n",
              "      <td>128.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11_softmax</th>\n",
              "      <td>-</td>\n",
              "      <td>[1, 13]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12_cls.Linear_0</th>\n",
              "      <td>[320, 2]</td>\n",
              "      <td>[1, 2]</td>\n",
              "      <td>642.0</td>\n",
              "      <td>640.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13_cls.Sigmoid_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[1, 2]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d7a6e332-0e1e-48ab-83bb-e570eb882935')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d7a6e332-0e1e-48ab-83bb-e570eb882935 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d7a6e332-0e1e-48ab-83bb-e570eb882935');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WSCNN(nn.Module):\n",
        "\n",
        "    def __init__(self, instance_len=40, merging='MAX', training=True):\n",
        "        super(WSCNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(4, 16, kernel_size=(1, 15), padding=(0, 7)),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=(1, 1)),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(32, 1, kernel_size=(1, 1)),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.pool1 = nn.MaxPool2d((instance_len, 1))\n",
        "\n",
        "        if merging == 'MAX':\n",
        "            self.pool2 = nn.AdaptiveMaxPool2d((1, 1))\n",
        "            # self.pool2 = nn.GlobalMaxPooling2d()\n",
        "        elif merging == 'AVG':\n",
        "            self.pool2 = nn.AdaptiveAvgPool2d((1, 1))\n",
        "            # self.pool2 = nn.GlobalAveragePooling2d()\n",
        "\n",
        "        self.cls = nn.Sequential(\n",
        "            nn.Linear(1, 2),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs, training=True, mask=None):\n",
        "        inputs = inputs.permute((0, 3, 2, 1))\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv3(x)\n",
        "        out = self.pool2(x)\n",
        "        out = self.cls(out)\n",
        "        return out[0][0],_\n",
        "\n",
        "x = torch.rand((1, 13, 40, 4))\n",
        "encoder = WSCNN()\n",
        "summary(encoder, x)"
      ],
      "metadata": {
        "id": "69lKk8-SSn89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845
        },
        "outputId": "682c775c-0f72-42b6-ea9f-4322e23ffd56"
      },
      "id": "69lKk8-SSn89",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================\n",
            "                    Kernel Shape     Output Shape Params Mult-Adds\n",
            "Layer                                                             \n",
            "0_conv1.Conv2d_0  [4, 16, 1, 15]  [1, 16, 40, 13]  976.0    499.2k\n",
            "1_conv1.ReLU_1                 -  [1, 16, 40, 13]      -         -\n",
            "2_pool1                        -   [1, 16, 1, 13]      -         -\n",
            "3_conv2.Conv2d_0  [16, 32, 1, 1]   [1, 32, 1, 13]  544.0    6.656k\n",
            "4_conv2.ReLU_1                 -   [1, 32, 1, 13]      -         -\n",
            "5_dropout                      -   [1, 32, 1, 13]      -         -\n",
            "6_conv3.Conv2d_0   [32, 1, 1, 1]    [1, 1, 1, 13]   33.0     416.0\n",
            "7_conv3.ReLU_1                 -    [1, 1, 1, 13]      -         -\n",
            "8_pool2                        -     [1, 1, 1, 1]      -         -\n",
            "9_cls.Linear_0            [1, 2]     [1, 1, 1, 2]    4.0       2.0\n",
            "10_cls.Sigmoid_1               -     [1, 1, 1, 2]      -         -\n",
            "--------------------------------------------------------------------\n",
            "                        Totals\n",
            "Total params            1.557k\n",
            "Trainable params        1.557k\n",
            "Non-trainable params       0.0\n",
            "Mult-Adds             506.274k\n",
            "====================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  df_sum = df.sum()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    Kernel Shape     Output Shape  Params  Mult-Adds\n",
              "Layer                                                               \n",
              "0_conv1.Conv2d_0  [4, 16, 1, 15]  [1, 16, 40, 13]   976.0   499200.0\n",
              "1_conv1.ReLU_1                 -  [1, 16, 40, 13]     NaN        NaN\n",
              "2_pool1                        -   [1, 16, 1, 13]     NaN        NaN\n",
              "3_conv2.Conv2d_0  [16, 32, 1, 1]   [1, 32, 1, 13]   544.0     6656.0\n",
              "4_conv2.ReLU_1                 -   [1, 32, 1, 13]     NaN        NaN\n",
              "5_dropout                      -   [1, 32, 1, 13]     NaN        NaN\n",
              "6_conv3.Conv2d_0   [32, 1, 1, 1]    [1, 1, 1, 13]    33.0      416.0\n",
              "7_conv3.ReLU_1                 -    [1, 1, 1, 13]     NaN        NaN\n",
              "8_pool2                        -     [1, 1, 1, 1]     NaN        NaN\n",
              "9_cls.Linear_0            [1, 2]     [1, 1, 1, 2]     4.0        2.0\n",
              "10_cls.Sigmoid_1               -     [1, 1, 1, 2]     NaN        NaN"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-738bc525-8789-4e56-a71a-4238f3272bfe\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_conv1.Conv2d_0</th>\n",
              "      <td>[4, 16, 1, 15]</td>\n",
              "      <td>[1, 16, 40, 13]</td>\n",
              "      <td>976.0</td>\n",
              "      <td>499200.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_conv1.ReLU_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[1, 16, 40, 13]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_pool1</th>\n",
              "      <td>-</td>\n",
              "      <td>[1, 16, 1, 13]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_conv2.Conv2d_0</th>\n",
              "      <td>[16, 32, 1, 1]</td>\n",
              "      <td>[1, 32, 1, 13]</td>\n",
              "      <td>544.0</td>\n",
              "      <td>6656.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_conv2.ReLU_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[1, 32, 1, 13]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5_dropout</th>\n",
              "      <td>-</td>\n",
              "      <td>[1, 32, 1, 13]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6_conv3.Conv2d_0</th>\n",
              "      <td>[32, 1, 1, 1]</td>\n",
              "      <td>[1, 1, 1, 13]</td>\n",
              "      <td>33.0</td>\n",
              "      <td>416.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7_conv3.ReLU_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[1, 1, 1, 13]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8_pool2</th>\n",
              "      <td>-</td>\n",
              "      <td>[1, 1, 1, 1]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9_cls.Linear_0</th>\n",
              "      <td>[1, 2]</td>\n",
              "      <td>[1, 1, 1, 2]</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10_cls.Sigmoid_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[1, 1, 1, 2]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-738bc525-8789-4e56-a71a-4238f3272bfe')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-738bc525-8789-4e56-a71a-4238f3272bfe button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-738bc525-8789-4e56-a71a-4238f3272bfe');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "# model = WeakRM().cuda()\n",
        "model = WSCNN().cuda()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.005)\n",
        "criterion = nn.BCELoss(weight=torch.tensor([0.8, 0.2])).cuda()\n",
        "num_steps = len(train_loader) * epochs\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_steps)\n",
        "warmup_scheduler = warmup.RAdamWarmup(optimizer)\n",
        "\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    # batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "    # training\n",
        "    model.train()\n",
        "    num_correct = 0\n",
        "    total_loss = 0\n",
        "    for i, (x, y) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x = x.float().cuda()\n",
        "        y = y.float().cuda()\n",
        "\n",
        "        outputs,_ = model(x)\n",
        "        \n",
        "        # print(outputs[0].shape)\n",
        "        # print(y[0].shape)\n",
        "        loss = criterion(outputs[0], y[0])\n",
        "\n",
        "        outputs = torch.argmax(outputs)\n",
        "\n",
        "        num_correct += int((outputs == torch.argmax(y)).sum())\n",
        "        total_loss += loss\n",
        "\n",
        "        train_acc = 100 * num_correct / ((i + 1) * batch_size)\n",
        "        train_loss = float(total_loss / (i + 1))\n",
        "    \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        with warmup_scheduler.dampening():\n",
        "            scheduler.step()\n",
        "            \n",
        "#         if i // 100 == 0:\n",
        "#             batch_bar.set_postfix(\n",
        "#                 acc=\"{:.04f}%\".format(train_acc),\n",
        "#                 loss=\"{:.04f}\".format(train_loss),\n",
        "#                 num_correct=num_correct,\n",
        "#                 lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "#             batch_bar.update()\n",
        "    \n",
        "#     batch_bar.close()\n",
        "\n",
        "    print_content =\"Epoch {}/{}: Train Acc {:.04f}%, Train Loss {:.04f}, Learning Rate {:.04f}\".format(\n",
        "        epoch,\n",
        "        epochs,\n",
        "        100 * num_correct / (len(train_loader) * batch_size),\n",
        "        float(total_loss / len(train_loader)),\n",
        "        float(optimizer.param_groups[0]['lr'])\n",
        "    )\n",
        "\n",
        "    print(print_content)\n",
        "\n",
        "    # eval\n",
        "    model.eval()\n",
        "    \n",
        "    num_correct = 0\n",
        "    total_loss = 0\n",
        "    predictions =[]\n",
        "    label = []\n",
        "    for i, (x, y) in enumerate(valid_loader):\n",
        "        x = x.float().cuda()\n",
        "        y = y.float().cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs_probs,_ = model(x)\n",
        "\n",
        "        outputs = torch.argmax(outputs_probs)\n",
        "\n",
        "        num_correct += int((outputs == torch.argmax(y)).sum())\n",
        "        total_loss += loss\n",
        "        \n",
        "        predictions.append(outputs_probs.detach().cpu().numpy()[0])\n",
        "        label.append(torch.argmax(y).detach().cpu().numpy())\n",
        "    # print(_)    \n",
        "    dev_acc = 100 * num_correct / len(valid_loader)\n",
        "    dev_loss = total_loss / len(valid_loader)\n",
        "        \n",
        "    auROC = roc_auc_score(np.array(label).flatten(), np.array(predictions)[:, 1])\n",
        "    print(\"Eval Acc {:.04f}%, Eval auROC {:.04f}%\".format(dev_acc, auROC))\n",
        "    \n",
        "#     torch.save({'epoch': epoch,'model_state_dict': model.state_dict(),'optimizer_state_dict': optimizer.state_dict()}, path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTliaIlmGkDb",
        "outputId": "5d1deeef-7e20-41ed-8308-4167e42342aa"
      },
      "id": "KTliaIlmGkDb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20: Train Acc 76.9533%, Train Loss 0.2579, Learning Rate 0.0010\n",
            "Eval Acc 80.0000%, Eval auROC 0.8389%\n",
            "Epoch 2/20: Train Acc 81.8500%, Train Loss 0.1893, Learning Rate 0.0010\n",
            "Eval Acc 82.7500%, Eval auROC 0.8465%\n",
            "Epoch 3/20: Train Acc 82.8200%, Train Loss 0.1860, Learning Rate 0.0009\n",
            "Eval Acc 83.1700%, Eval auROC 0.8458%\n",
            "Epoch 4/20: Train Acc 82.5233%, Train Loss 0.1860, Learning Rate 0.0009\n",
            "Eval Acc 84.0300%, Eval auROC 0.8478%\n",
            "Epoch 5/20: Train Acc 82.7367%, Train Loss 0.1854, Learning Rate 0.0009\n",
            "Eval Acc 83.8200%, Eval auROC 0.8434%\n",
            "Epoch 6/20: Train Acc 82.6700%, Train Loss 0.1853, Learning Rate 0.0008\n",
            "Eval Acc 84.0000%, Eval auROC 0.8454%\n",
            "Epoch 7/20: Train Acc 82.8000%, Train Loss 0.1851, Learning Rate 0.0007\n",
            "Eval Acc 83.0700%, Eval auROC 0.8451%\n",
            "Epoch 8/20: Train Acc 82.6133%, Train Loss 0.1846, Learning Rate 0.0007\n",
            "Eval Acc 83.9300%, Eval auROC 0.8455%\n",
            "Epoch 9/20: Train Acc 82.8967%, Train Loss 0.1834, Learning Rate 0.0006\n",
            "Eval Acc 83.7600%, Eval auROC 0.8484%\n",
            "Epoch 10/20: Train Acc 82.7733%, Train Loss 0.1841, Learning Rate 0.0005\n",
            "Eval Acc 83.7800%, Eval auROC 0.8470%\n",
            "Epoch 11/20: Train Acc 83.1167%, Train Loss 0.1822, Learning Rate 0.0004\n",
            "Eval Acc 83.9800%, Eval auROC 0.8499%\n",
            "Epoch 12/20: Train Acc 83.0933%, Train Loss 0.1818, Learning Rate 0.0003\n",
            "Eval Acc 84.0600%, Eval auROC 0.8501%\n",
            "Epoch 13/20: Train Acc 82.9933%, Train Loss 0.1811, Learning Rate 0.0003\n",
            "Eval Acc 84.3200%, Eval auROC 0.8518%\n",
            "Epoch 14/20: Train Acc 83.1933%, Train Loss 0.1800, Learning Rate 0.0002\n",
            "Eval Acc 84.0000%, Eval auROC 0.8513%\n",
            "Epoch 15/20: Train Acc 83.3700%, Train Loss 0.1793, Learning Rate 0.0001\n",
            "Eval Acc 84.3400%, Eval auROC 0.8529%\n",
            "Epoch 16/20: Train Acc 83.4000%, Train Loss 0.1784, Learning Rate 0.0001\n",
            "Eval Acc 84.4000%, Eval auROC 0.8520%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "ZwbrkD9MtjyT",
        "outputId": "10b6204c-9469-4039-9117-146adb840f4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1220, 0.1194, 0.0968, 0.0888, 0.0895, 0.0618, 0.0523, 0.0650, 0.0534,\n",
            "         0.0582, 0.0610, 0.0676, 0.0642]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Epoch 1/20: Train Acc 100.0000%, Train Loss 0.1773, Learning Rate 0.0010 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-96-e87300c2f767>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mauROC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;31m# wandb.log({\"Train Acc:\":train_acc, \"Train loss:\": train_loss,\"Test Acc \":dev_acc,\"Test loss\":dev_loss,\"auROC\":auROC,\"lr\":lr})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    794\u001b[0m             raise ValueError(\n\u001b[1;32m    795\u001b[0m                 \u001b[0;34m\"Found array with dim %d. %s expected <= 2.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m                 \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m             )\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. Estimator expected <= 2."
          ]
        }
      ],
      "source": [
        "# from tqdm import tqdm\n",
        "# from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef\n",
        "# from sklearn.metrics import precision_score, recall_score\n",
        "# from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "# model = WeakRM().cuda()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.005)\n",
        "# # criterion = nn.BCELoss()\n",
        "# criterion = nn.BCELoss(weight=torch.tensor([0.8, 0.2])).cuda()\n",
        "# num_steps = len(train_loader) * epochs\n",
        "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_steps)\n",
        "# # warmup_scheduler = warmup.RAdamWarmup(optimizer)\n",
        "\n",
        "\n",
        "# for epoch in range(1, epochs + 1):\n",
        "#     batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "#     num_correct = 0\n",
        "#     total_loss = 0\n",
        "\n",
        "#     # training samples\n",
        "#     for i, (x, y) in enumerate(train_loader):\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         x = x.float().cuda()\n",
        "#         y = y.float().cuda()\n",
        "\n",
        "#         outputs,_ = model(x)\n",
        "\n",
        "#         # print(outputs[0].shape)\n",
        "#         # print(y.shape)\n",
        "\n",
        "#         loss = criterion(outputs[0], y[0])\n",
        "\n",
        "#         outputs = torch.argmax(outputs)\n",
        "\n",
        "#         # outputs = torch.tensor([0 if outputs[0][0]<0.5 else 1]).cuda()\n",
        "\n",
        "#         num_correct += int((outputs == y).sum())\n",
        "#         total_loss += float(loss)\n",
        "\n",
        "#         train_acc = 100 * num_correct / ((i + 1) * batch_size)\n",
        "#         train_loss = float(total_loss / (i + 1))\n",
        "        \n",
        "#         # tqdm lets you add some details so you can monitor training as you train.\n",
        "#         batch_bar.set_postfix(\n",
        "#             acc=\"{:.04f}%\".format(train_acc),\n",
        "#             loss=\"{:.04f}\".format(train_loss),\n",
        "#             num_correct=num_correct,\n",
        "#             lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "\n",
        "#         # Another couple things you need for FP16.\n",
        "#         loss.backward()  # This is a replacement for loss.backward()\n",
        "#         optimizer.step()  # This is a replacement for optimizer.step()\n",
        "#         # with warmup_scheduler.dampening():\n",
        "#         #     scheduler.step()\n",
        "    \n",
        "#         batch_bar.update()  # Update tqdm bar\n",
        "#     print(_)\n",
        "    \n",
        "#     batch_bar.close()  # You need this to close the tqdm bar\n",
        "\n",
        "#     print_content =\"Epoch {}/{}: Train Acc {:.04f}%, Train Loss {:.04f}, Learning Rate {:.04f} \\n\".format(\n",
        "#     epoch,\n",
        "#     epochs,\n",
        "#     100 * num_correct / (len(train_loader) * batch_size),\n",
        "#     float(total_loss / len(train_loader)),\n",
        "#     float(optimizer.param_groups[0]['lr']))\n",
        "\n",
        "#     print(print_content)\n",
        "\n",
        "#     batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Eval')\n",
        "\n",
        "#     num_correct = 0\n",
        "#     total_loss = 0\n",
        "\n",
        "#     # eval samples\n",
        "#     model.eval()\n",
        "#     predictions =[]\n",
        "#     label = []\n",
        "#     for i, (x, y) in enumerate(valid_loader):\n",
        "#         x = x.float().cuda()\n",
        "#         y = y.float().cuda()\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             outputs,_ = model(x)\n",
        "\n",
        "#         # outputs = torch.tensor([0 if outputs[0][0]<0.5 else 1]).cuda()\n",
        "#         # print(outputs)\n",
        "\n",
        "#         outputs = torch.argmax(outputs)\n",
        "#         num_correct += int((outputs == y).sum())\n",
        "#         total_loss += float(loss)\n",
        "        \n",
        "#         predictions.append(outputs.detach().cpu().numpy())\n",
        "#         label.append(y.detach().cpu().numpy())\n",
        "\n",
        "\n",
        "#         dev_acc = 100 * num_correct / ((i + 1) * batch_size)\n",
        "#         dev_loss = float(total_loss / (i + 1))\n",
        "#         # print(dev_loss)\n",
        "\n",
        "#         # tqdm lets you add some details so you can monitor training as you train.\n",
        "#         batch_bar.set_postfix(\n",
        "#             acc=\"{:.04f}%\".format(dev_acc),\n",
        "#             loss=\"{:.04f}\".format(dev_loss),\n",
        "#             num_correct=num_correct,\n",
        "#             lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "\n",
        "#         # Another couple things you need for FP16.\n",
        "#         batch_bar.update()  # Update tqdm bar\n",
        "\n",
        "#     batch_bar.close()  # You need this to close the tqdm bar\n",
        "\n",
        "\n",
        "#     auROC = roc_auc_score(y_true=label, y_score=predictions)\n",
        "\n",
        "#     # wandb.log({\"Train Acc:\":train_acc, \"Train loss:\": train_loss,\"Test Acc \":dev_acc,\"Test loss\":dev_loss,\"auROC\":auROC,\"lr\":lr})\n",
        "#     print(auROC)\n",
        "#     print_content =\"Epoch {}/{}: Eval Acc {:.04f}%, Eval auROC {:.04f}%\\n\".format(\n",
        "#     epoch,\n",
        "#     epochs,\n",
        "#     dev_acc,\n",
        "#     auROC)\n",
        "#     print(print_content)\n",
        "    \n",
        "#     path = data_path+f\"/model_epoch_{epoch}.ckpt\"\n",
        "#     torch.save({'epoch': epoch,'model_state_dict': model.state_dict(),'optimizer_state_dict': optimizer.state_dict()}, path)\n",
        "\n"
      ],
      "id": "ZwbrkD9MtjyT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensorflow version"
      ],
      "metadata": {
        "id": "nAjYsOnLR5ET"
      },
      "id": "nAjYsOnLR5ET"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "byHmNhx0VrK5"
      },
      "id": "byHmNhx0VrK5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3a8c1680-a75f-49c8-a5cd-ae1aac217b58"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.activations import sigmoid\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import Input\n",
        "\n",
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "tfkc = tf.keras.callbacks\n",
        "\n",
        "\n",
        "class Noisyand(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, a=7.5, **kwargs):\n",
        "        super(Noisyand, self).__init__(**kwargs)\n",
        "        self.a = a\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.b = self.add_weight(shape=(input_shape[-1],),\n",
        "                                 initializer=tfk.initializers.RandomUniform(minval=0, maxval=1),\n",
        "                                 constraint=tfk.constraints.MinMaxNorm(min_value=0.0, max_value=1.0),\n",
        "                                 trainable=True)\n",
        "\n",
        "    def call(self, inputs, training=True, mask=None):\n",
        "        if len(inputs.shape) == 4:\n",
        "            inputs = tf.squeeze(inputs, 2)\n",
        "        part1 = sigmoid((tf.reduce_mean(inputs, axis=1) - self.b) * self.a) - sigmoid(-self.a * self.b)\n",
        "        part2 = sigmoid(self.a * (1 - self.b)) - sigmoid(-self.a * self.b)\n",
        "        return part1 / part2\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], input_shape[-1]\n",
        "\n",
        "\n",
        "class WSCNN(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, instance_len=40, merging='MAX', a=7.5):\n",
        "        super(WSCNN, self).__init__()\n",
        "\n",
        "        assert merging in ['MAX', 'AVG', 'NOISY']\n",
        "\n",
        "        self.conv1 = tfkl.Conv2D(16, (1, 15), padding='same',\n",
        "                                 activation='relu', kernel_regularizer=l2(0.005))\n",
        "        self.conv2 = tfkl.Conv2D(32, (1, 1), padding='same',\n",
        "                                 activation='relu', kernel_regularizer=l2(0.005))\n",
        "        self.conv3 = tfkl.Conv2D(1, (1, 1), padding='same',\n",
        "                                 activation='sigmoid', kernel_regularizer=l2(0.005))\n",
        "        self.dropout = tfkl.Dropout(0.5)\n",
        "        self.pool1 = tfkl.MaxPool2D(pool_size=(1, instance_len))\n",
        "\n",
        "        if merging == 'MAX':\n",
        "            self.pool2 = tfkl.GlobalMaxPooling2D()\n",
        "        elif merging == 'AVG':\n",
        "            self.pool2 = tfkl.GlobalAveragePooling2D()\n",
        "        elif merging == 'NOISY':\n",
        "            self.pool2 = Noisyand(a=a)\n",
        "\n",
        "    def call(self, inputs, training=True, mask=None):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        if training:\n",
        "            x = self.dropout(x, training=training)\n",
        "        x = self.conv3(x)\n",
        "        out = self.pool2(x)\n",
        "        return out\n",
        "\n",
        "    def model(self):\n",
        "        x = Input(shape=(13, 40, 5))\n",
        "        return tf.keras.Model(inputs=[x], outputs=self.call(x))\n",
        "\n",
        "\n",
        "class WSCNNLSTM(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, merging='MAX', a=7.5):\n",
        "        super(WSCNNLSTM, self).__init__()\n",
        "\n",
        "        assert merging in ['MAX', 'AVG', 'NOISY']  ## 如果merging不是在这几个值之间，那么就会报错\n",
        "\n",
        "        # 2D, batch = 1, 确保每一个输入的instance都是共享参数的CNN提取feature\n",
        "        self.conv1 = tf.keras.layers.Conv2D(16, (1, 15), padding='same', activation='relu',\n",
        "                                            kernel_regularizer=l2(0.005))\n",
        "        self.conv2 = tf.keras.layers.Conv1D(1, 1, padding='same', activation='sigmoid',\n",
        "                                            kernel_regularizer=l2(0.005))\n",
        "        self.lstm = tf.keras.layers.TimeDistributed(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)))\n",
        "        self.dropout1 = tfkl.Dropout(0.2)\n",
        "        self.dropout2 = tfkl.Dropout(0.5)\n",
        "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=(1, 5))\n",
        "        if merging == 'MAX':\n",
        "            self.pool2 = tfkl.GlobalMaxPooling1D()\n",
        "        elif merging == 'AVG':\n",
        "            self.pool2 = tfkl.GlobalAveragePooling1D()\n",
        "        elif merging == 'NOISY':\n",
        "            self.pool2 = Noisyand(a=a)\n",
        "\n",
        "    def call(self, inputs, training=True, mask=None):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.pool1(x)\n",
        "        if training:\n",
        "            x = self.dropout1(x, training=training)\n",
        "        x = self.lstm(x)\n",
        "        if training:\n",
        "            x = self.dropout2(x, training=training)\n",
        "        x = self.conv2(x)\n",
        "        out = self.pool2(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def model(self):\n",
        "        x = Input(shape=(13, 40, 4))\n",
        "        return tf.keras.Model(inputs=[x], outputs=self.call(x))\n",
        "\n",
        "\n",
        "class WeakRM(tf.keras.Model):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(WeakRM, self).__init__()\n",
        "        # 1D batch = 6, 保证每一条instance的参数都是由CNN来提取的\n",
        "        self.conv1 = tfkl.Conv1D(32, 15, padding='same', activation='relu')\n",
        "        self.conv2 = tfkl.Conv1D(16, 5, padding='same', activation='relu',\n",
        "                                 kernel_regularizer=l2(0.005))\n",
        "        self.dropout = tfkl.Dropout(0.2)\n",
        "        self.pool1 = tfkl.MaxPool1D(pool_size=2)\n",
        "\n",
        "        self.att_v = tfkl.Dense(128, activation='tanh')\n",
        "        self.att_u = tfkl.Dense(128, activation='sigmoid')\n",
        "\n",
        "        self.attention_weights = tfkl.Dense(1)\n",
        "\n",
        "        self.classifier = tfkl.Dense(1, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs, training=True, mask=None):\n",
        "        #   (1,none, instance_length, 4)\n",
        "        input_bag = tf.squeeze(inputs, axis=0)\n",
        "        ## squeeze function is to remove all the shape equal 1\n",
        "\n",
        "        ## so we could use the convolutional\n",
        "        inst_conv1 = self.conv1(input_bag)\n",
        "        inst_pool1 = self.pool1(inst_conv1)\n",
        "        inst_pool1 = self.dropout(inst_pool1, training=training)\n",
        "\n",
        "        inst_conv2 = self.conv2(inst_pool1)\n",
        "\n",
        "        inst_features = tfkl.Flatten()(inst_conv2)\n",
        "\n",
        "        attention_vmatrix = self.att_v(inst_features)\n",
        "        attention_umatrix = self.att_u(inst_features)\n",
        "\n",
        "        gated_attention = self.attention_weights(attention_vmatrix * attention_umatrix)\n",
        "        print(gated_attention.shape)\n",
        "\n",
        "        gated_attention = tf.transpose(gated_attention, perm=[1, 0])\n",
        "        gated_attention = tfkl.Softmax()(gated_attention)\n",
        "\n",
        "        bag_features = tf.matmul(gated_attention, inst_features)\n",
        "\n",
        "        bag_probability = self.classifier(bag_features)\n",
        "\n",
        "        return bag_probability, gated_attention\n",
        "\n",
        "    # def model(self):\n",
        "    #     x = Input(shape=(13, 40, 5))\n",
        "    #     return tf.keras.Model(inputs=[x], outputs=self.call(x))\n",
        "\n",
        "\n",
        "class WeakRMLSTM(tf.keras.Model):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(WeakRMLSTM, self).__init__()\n",
        "\n",
        "        self.conv1 = tf.keras.layers.Conv2D(16, (1, 15), padding='same', activation='relu')\n",
        "        # self.conv1 = tfkl.Conv1D(32, 15, padding='same', activation='relu')\n",
        "        # self.conv2 = tfkl.Conv1D(16, 5, padding='same', activation='relu',\n",
        "        #                          kernel_regularizer=l2(0.005))\n",
        "        self.lstm = tf.keras.layers.TimeDistributed(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)))\n",
        "        self.dropout = tfkl.Dropout(0.2)\n",
        "        self.pool1 = tfkl.MaxPool2D(pool_size=(1, 2))\n",
        "\n",
        "        self.att_v = tfkl.Dense(128, activation='tanh')\n",
        "        self.att_u = tfkl.Dense(128, activation='sigmoid')\n",
        "\n",
        "        self.attention_weights = tfkl.Dense(1)\n",
        "\n",
        "        self.classifier = tfkl.Dense(1, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs, training=True, mask=None):\n",
        "        # input_bag = tf.squeeze(inputs, axis=0)\n",
        "        input_bag = inputs\n",
        "        inst_conv1 = self.conv1(input_bag)\n",
        "        inst_pool1 = self.pool1(inst_conv1)\n",
        "        if training:\n",
        "            inst_pool1 = self.dropout(inst_pool1, training=training)\n",
        "\n",
        "        inst_conv2 = self.lstm(inst_pool1)\n",
        "\n",
        "        inst_features = tf.squeeze(inst_conv2, axis=0)\n",
        "\n",
        "        attention_vmatrix = self.att_v(inst_features)\n",
        "        attention_umatrix = self.att_u(inst_features)\n",
        "\n",
        "        gated_attention = self.attention_weights(attention_vmatrix * attention_umatrix)\n",
        "\n",
        "        gated_attention = tf.transpose(gated_attention, perm=[1, 0])\n",
        "        gated_attention = tfkl.Softmax()(gated_attention)\n",
        "\n",
        "        # tf.stop_gradient(gated_attention)\n",
        "\n",
        "        bag_features = tf.matmul(gated_attention, inst_features)\n",
        "\n",
        "        bag_probability = self.classifier(bag_features)\n",
        "\n",
        "        return bag_probability, gated_attention\n",
        "\n",
        "    def model(self):\n",
        "        x = Input(shape=(13, 40, 5))\n",
        "        return tf.keras.Model(inputs=[x], outputs=self.call(x))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "======================================================================================================\n",
        "ADDING THE STRUCTURE INFORMATION INTO THE MODEL\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class WeakRM_struc(tf.keras.Model):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(WeakRM_struc, self).__init__()\n",
        "        # 1D batch = 6, 保证每一条instance的参数都是由CNN来提取的\n",
        "        self.conv1 = tfkl.Conv1D(32, 15, padding='same', activation='relu')\n",
        "        self.conv2 = tfkl.Conv1D(16, 5, padding='same', activation='relu',\n",
        "                                 kernel_regularizer=l2(0.005))\n",
        "\n",
        "        self.conv1_str = tfkl.Conv1D(32, 15, padding='same', activation='relu')\n",
        "        self.conv2_str = tfkl.Conv1D(16, 5, padding='same', activation='relu',\n",
        "                                     kernel_regularizer=l2(0.005))\n",
        "\n",
        "        self.dropout = tfkl.Dropout(0.2)\n",
        "        self.pool1 = tfkl.MaxPool1D(pool_size=2)\n",
        "\n",
        "        self.att_v = tfkl.Dense(128, activation='tanh')\n",
        "        self.att_u = tfkl.Dense(128, activation='sigmoid')\n",
        "\n",
        "        self.attention_weights = tfkl.Dense(1)\n",
        "\n",
        "        self.classifier = tfkl.Dense(1, activation='sigmoid')\n",
        "\n",
        "    def call(self, train_seq, train_str, training=True, mask=None):\n",
        "        ###############################################\n",
        "        ### sequence sequence\n",
        "        ###############################################\n",
        "        #\n",
        "        input_bag_seq = tf.squeeze(train_seq, axis=0)\n",
        "        inst_conv1_seq = self.conv1(input_bag_seq)\n",
        "        inst_pool1_seq = self.pool1(inst_conv1_seq)\n",
        "        inst_pool1_seq = self.dropout(inst_pool1_seq, training=training)\n",
        "\n",
        "        inst_conv2_seq = self.conv2(inst_pool1_seq)\n",
        "        inst_features_seq = tfkl.Flatten()(inst_conv2_seq)\n",
        "\n",
        "        ###############################################\n",
        "        ### structure sequence\n",
        "        ###############################################\n",
        "\n",
        "        input_bag_str = tf.squeeze(train_str, axis=0)\n",
        "        inst_conv1_str = self.conv1_str(input_bag_str)\n",
        "        inst_pool1_str = self.pool1(inst_conv1_str)\n",
        "        inst_pool1_str = self.dropout(inst_pool1_str, training=training)\n",
        "\n",
        "        inst_conv2_str = self.conv2_str(inst_pool1_str)\n",
        "        inst_features_str = tfkl.Flatten()(inst_conv2_str)\n",
        "        # inst_features = tfkl.Flatten()(inst_conv2_str)\n",
        "\n",
        "        ###############################################\n",
        "        ### combine two type sequence\n",
        "        ###############################################\n",
        "\n",
        "        inst_features = tf.concat([inst_features_seq, inst_features_str], 0)\n",
        "\n",
        "        attention_vmatrix = self.att_v(inst_features)\n",
        "        attention_umatrix = self.att_u(inst_features)\n",
        "\n",
        "        gated_attention = self.attention_weights(attention_vmatrix * attention_umatrix)\n",
        "\n",
        "        gated_attention = tf.transpose(gated_attention, perm=[1, 0])\n",
        "        gated_attention = tfkl.Softmax()(gated_attention)\n",
        "\n",
        "        bag_features = tf.matmul(gated_attention, inst_features)\n",
        "\n",
        "        bag_probability = self.classifier(bag_features)\n",
        "\n",
        "        return bag_probability, gated_attention\n",
        "\n",
        "\n",
        "class WeakRM_struc_only(tf.keras.Model):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(WeakRM_struc_only, self).__init__()\n",
        "        self.conv1 = tfkl.Conv1D(32, 15, padding='same', activation='relu')\n",
        "        self.conv2 = tfkl.Conv1D(16, 5, padding='same', activation='relu',\n",
        "                                 kernel_regularizer=l2(0.005))\n",
        "\n",
        "        self.conv1_str = tfkl.Conv1D(32, 15, padding='same', activation='relu')\n",
        "        self.conv2_str = tfkl.Conv1D(16, 5, padding='same', activation='relu',\n",
        "                                     kernel_regularizer=l2(0.005))\n",
        "\n",
        "        self.lstm = tf.keras.layers.LSTM(16)\n",
        "\n",
        "        self.dropout = tfkl.Dropout(0.2)\n",
        "        self.pool1 = tfkl.MaxPool1D(pool_size=2)\n",
        "\n",
        "        self.att_v = tfkl.Dense(128, activation='tanh')\n",
        "        self.att_u = tfkl.Dense(128, activation='sigmoid')\n",
        "\n",
        "        self.attention_weights = tfkl.Dense(1)\n",
        "\n",
        "        self.classifier = tfkl.Dense(1, activation='sigmoid')\n",
        "\n",
        "    def call(self, train_seq, train_str, training=True, mask=None):\n",
        "        ###############################################\n",
        "        ### sequence sequence\n",
        "        ###############################################\n",
        "        #\n",
        "        input_bag_seq = tf.squeeze(train_seq, axis=0)\n",
        "        inst_conv1_seq = self.conv1(input_bag_seq)\n",
        "        inst_pool1_seq = self.pool1(inst_conv1_seq)\n",
        "        inst_pool1_seq = self.dropout(inst_pool1_seq, training=training)\n",
        "\n",
        "        inst_conv2_seq = self.conv2(inst_pool1_seq)\n",
        "        inst_features_seq = tfkl.Flatten()(inst_conv2_seq)\n",
        "\n",
        "        attention_vmatrix_seq = self.att_v(inst_features_seq)\n",
        "        attention_umatrix_seq = self.att_u(inst_features_seq)\n",
        "\n",
        "        gated_attention_seq = self.attention_weights(attention_vmatrix_seq * attention_umatrix_seq)\n",
        "\n",
        "        gated_attention_seq = tf.transpose(gated_attention_seq, perm=[1, 0])\n",
        "        gated_attention_seq = tfkl.Softmax()(gated_attention_seq)\n",
        "\n",
        "        bag_features_seq = tf.matmul(gated_attention_seq, inst_features_seq)\n",
        "\n",
        "        ###############################################\n",
        "        ### structure sequence\n",
        "        ###############################################\n",
        "\n",
        "        # input_bag_str = tf.squeeze(train_str, axis=0)\n",
        "        inst_conv1_str = self.conv1_str(train_str)\n",
        "        inst_pool1_str = self.pool1(inst_conv1_str)\n",
        "        inst_pool1_str = self.dropout(inst_pool1_str, training=training)\n",
        "\n",
        "        inst_conv2_str = self.lstm(inst_pool1_str)\n",
        "        # inst_features_str = tfkl.Flatten()(inst_conv2_str)\n",
        "        # inst_conv2_str = tfkl.Flatten()(inst_pool1_str)\n",
        "\n",
        "        ###############################################\n",
        "        ### combine two type sequence\n",
        "        ###############################################\n",
        "\n",
        "        bag_features_cbd = tf.concat([bag_features_seq, inst_conv2_str], 1)\n",
        "\n",
        "        # bag_probability = self.classifier(bag_features_cbd)\n",
        "        bag_probability = self.classifier(bag_features_cbd)\n",
        "\n",
        "        # return bag_probability, gated_attention_seq\n",
        "        return bag_probability, inst_conv2_str\n",
        "\n",
        "\n",
        "# model = WeakRM()\n",
        "# # model = WeakRMLSTM()\n",
        "# # model = WSCNN()\n",
        "\n",
        "# # model.build(input_shape=(1, None, 40, 4))\n",
        "# # print(model.summary())\n",
        "# print(model.model().summary())"
      ],
      "id": "3a8c1680-a75f-49c8-a5cd-ae1aac217b58"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF data preparation"
      ],
      "metadata": {
        "id": "Sp64ScDPT-cV"
      },
      "id": "Sp64ScDPT-cV"
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_path = \"/content/drive/MyDrive/iDeepS-master/datasets/clip/10_PARCLIP_ELAVL1A_hg19/30000/training_sample_0/sequences.fa.gz\"\n",
        "valid_data_path = \"/content/drive/MyDrive/iDeepS-master/datasets/clip/10_PARCLIP_ELAVL1A_hg19/30000/test_sample_0/sequences.fa.gz\"\n",
        "\n",
        "train_data = read_file_helper(train_data_path)\n",
        "train_data[\"seq\"] = train_data[\"seq_part1\"] + train_data[\"seq_part2\"]\n",
        "train_data, train_label = create_bag(train_data[\"seq\"]), train_data[\"y\"].to_numpy().reshape(-1,1)\n",
        "\n",
        "# print(train_data)\n",
        "# print(train_label)"
      ],
      "metadata": {
        "id": "hgrUm76NUEdR"
      },
      "id": "hgrUm76NUEdR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_data_path = \"/content/drive/MyDrive/iDeepS-master/datasets/clip/10_PARCLIP_ELAVL1A_hg19/30000/test_sample_0/sequences.fa.gz\"\n",
        "valid_data = read_file_helper(valid_data_path)\n",
        "valid_data[\"seq\"] = valid_data[\"seq_part1\"] + valid_data[\"seq_part2\"]\n",
        "valid_data, valid_label = create_bag(valid_data[\"seq\"]), valid_data[\"y\"].to_numpy().reshape(-1,1)\n"
      ],
      "metadata": {
        "id": "2sEbcotJWKtE"
      },
      "id": "2sEbcotJWKtE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(valid_data)\n",
        "print(valid_data.shape)\n",
        "print(train_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySCA8Ps9WbO3",
        "outputId": "7b75f21e-fe5b-49a5-a20a-7c93cc1d9c38"
      },
      "id": "ySCA8Ps9WbO3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 13, 40, 4)\n",
            "(30000, 13, 40, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(train_label[0]))\n",
        "train_label.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYDPOVhFojsE",
        "outputId": "0c9950a6-2889-452a-b7b6-08de27e56b16"
      },
      "id": "oYDPOVhFojsE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30000, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(valid_data)\n",
        "x = []\n",
        "for i in range(len(valid_data)):\n",
        "  x.append(valid_data[i])\n",
        "x = np.asarray(x)"
      ],
      "metadata": {
        "id": "YsRmOf47qIA9"
      },
      "id": "YsRmOf47qIA9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import itertools\n",
        "import os\n",
        "import time\n",
        "\n",
        "instance_len =40\n",
        "train_dataset = tf.data.Dataset.from_generator(lambda: itertools.zip_longest(train_data, train_label),\n",
        "                                               output_types=(tf.float32, tf.int32),\n",
        "                                               output_shapes=(tf.TensorShape([None, instance_len, 4]),\n",
        "                                                              tf.TensorShape([None])))\n",
        "valid_dataset = tf.data.Dataset.from_generator(lambda: itertools.zip_longest(valid_data, valid_label),\n",
        "                                               output_types=(tf.float32, tf.int32),\n",
        "                                               output_shapes=(tf.TensorShape([None, instance_len, 4]),\n",
        "                                                              tf.TensorShape([None])))\n",
        "train_dataset = train_dataset.shuffle(100).batch(1)\n",
        "valid_dataset = valid_dataset.batch(1)\n",
        "\n"
      ],
      "metadata": {
        "id": "aFv2F7cgSD1X"
      },
      "id": "aFv2F7cgSD1X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in train_dataset:\n",
        "  print(x,y)\n",
        "  break"
      ],
      "metadata": {
        "id": "Xr2VybZhYnSu"
      },
      "id": "Xr2VybZhYnSu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.activations import sigmoid\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import Input\n",
        "\n",
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "tfkc = tf.keras.callbacks\n",
        "class WeakRM(tf.keras.Model):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(WeakRM, self).__init__()\n",
        "        # 1D batch = 6, 保证每一条instance的参数都是由CNN来提取的\n",
        "        self.conv1 = tfkl.Conv1D(32, 15, padding='same', activation='relu')\n",
        "        self.conv2 = tfkl.Conv1D(16, 5, padding='same', activation='relu',\n",
        "                                 kernel_regularizer=l2(0.005))\n",
        "        self.dropout = tfkl.Dropout(0.2)\n",
        "        self.pool1 = tfkl.MaxPool1D(pool_size=2)\n",
        "\n",
        "        self.att_v = tfkl.Dense(128, activation='tanh')\n",
        "        self.att_u = tfkl.Dense(128, activation='sigmoid')\n",
        "\n",
        "        self.attention_weights = tfkl.Dense(1)\n",
        "\n",
        "        self.classifier = tfkl.Dense(1, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs, training=True, mask=None):\n",
        "        #   (1,none, instance_length, 4)\n",
        "        input_bag = tf.squeeze(inputs, axis=0)\n",
        "        ## squeeze function is to remove all the shape equal 1\n",
        "\n",
        "        ## so we could use the convolutional\n",
        "        inst_conv1 = self.conv1(input_bag)\n",
        "        inst_pool1 = self.pool1(inst_conv1)\n",
        "        inst_pool1 = self.dropout(inst_pool1, training=training)\n",
        "\n",
        "        inst_conv2 = self.conv2(inst_pool1)\n",
        "\n",
        "        inst_features = tfkl.Flatten()(inst_conv2)\n",
        "\n",
        "        attention_vmatrix = self.att_v(inst_features)\n",
        "        attention_umatrix = self.att_u(inst_features)\n",
        "\n",
        "        gated_attention = self.attention_weights(attention_vmatrix * attention_umatrix)\n",
        "        print(gated_attention)\n",
        "\n",
        "        gated_attention = tf.transpose(gated_attention, perm=[1, 0])\n",
        "        gated_attention = tfkl.Softmax()(gated_attention)\n",
        "        # print(np.array(gated_attention))\n",
        "        # print(np.sum(gated_attention))\n",
        "\n",
        "        bag_features = tf.matmul(gated_attention, inst_features)\n",
        "\n",
        "        bag_probability = self.classifier(bag_features)\n",
        "\n",
        "        return bag_probability, gated_attention\n",
        "    def model(self):\n",
        "        x = Input(shape=(13, 40, 5))\n",
        "        return tf.keras.Model(inputs=[x], outputs=self.call(x))\n",
        "\n",
        "class WSCNN(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, instance_len=40, merging='MAX', a=7.5):\n",
        "        super(WSCNN, self).__init__()\n",
        "\n",
        "        assert merging in ['MAX', 'AVG', 'NOISY']\n",
        "\n",
        "        self.conv1 = tfkl.Conv2D(16, (1, 15), padding='same',\n",
        "                                 activation='relu', kernel_regularizer=l2(0.005))\n",
        "        self.conv2 = tfkl.Conv2D(32, (1, 1), padding='same',\n",
        "                                 activation='relu', kernel_regularizer=l2(0.005))\n",
        "        self.conv3 = tfkl.Conv2D(1, (1, 1), padding='same',\n",
        "                                 activation='sigmoid', kernel_regularizer=l2(0.005))\n",
        "        self.dropout = tfkl.Dropout(0.5)\n",
        "        self.pool1 = tfkl.MaxPool2D(pool_size=(1, instance_len))\n",
        "\n",
        "        if merging == 'MAX':\n",
        "            self.pool2 = tfkl.GlobalMaxPooling2D()\n",
        "        elif merging == 'AVG':\n",
        "            self.pool2 = tfkl.GlobalAveragePooling2D()\n",
        "        elif merging == 'NOISY':\n",
        "            self.pool2 = Noisyand(a=a)\n",
        "\n",
        "    def call(self, inputs, training=True, mask=None):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        if training:\n",
        "            x = self.dropout(x, training=training)\n",
        "        x = self.conv3(x)\n",
        "        out = self.pool2(x)\n",
        "        return out\n",
        "\n",
        "    def model(self):\n",
        "        x = Input(shape=(13, 40, 5))\n",
        "        return tf.keras.Model(inputs=[x], outputs=self.call(x))\n",
        "\n",
        "# model = WeakRM()\n",
        "model = WSCNN()\n",
        "print(model.model().summary())\n",
        "\n"
      ],
      "metadata": {
        "id": "8ud_kxCZ4Rw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcc78e0d-5e3b-437f-b91f-56785439e2d3"
      },
      "id": "8ud_kxCZ4Rw8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 13, 40, 5)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 13, 40, 16)        1216      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 13, 1, 16)        0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 13, 1, 32)         544       \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 13, 1, 32)         0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 13, 1, 1)          33        \n",
            "                                                                 \n",
            " global_max_pooling2d (Globa  (None, 1)                0         \n",
            " lMaxPooling2D)                                                  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,793\n",
            "Trainable params: 1,793\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys \n",
        "# set the optimization\n",
        "opt = tf.keras.optimizers.Adam(lr=5e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=1e-5)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean()\n",
        "valid_loss = tf.keras.metrics.Mean()\n",
        "train_auROC = tf.keras.metrics.AUC()\n",
        "valid_auROC = tf.keras.metrics.AUC()\n",
        "\n",
        "## Specify the input size\n",
        "\n",
        "# model = WeakRM()\n",
        "model = WSCNN()\n",
        "\n",
        "instance_len =40\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(1, None, instance_len, 4), dtype=tf.float32),\n",
        "    tf.TensorSpec(shape=(1, 1), dtype=tf.int32)\n",
        "]\n",
        "\n",
        "# @tf.function(input_signature=train_step_signature)\n",
        "def train_step(train_seq, train_label):\n",
        "    with tf.GradientTape() as tape:\n",
        "        output_probs, _ = model(train_seq, training=True)\n",
        "        # print(\"Train Gated Attenion:\",_)\n",
        "        # print(_.shape)\n",
        "        # attention = _.numpy()\n",
        "        # print(attention)\n",
        "        # tf.print(_ ,output_stream=sys.stdout)\n",
        "\n",
        "        loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)(y_true=train_label, y_pred=output_probs)\n",
        "        total_loss = loss + tf.reduce_sum(model.losses)  ## calculate the total_loss of each tensor (including the l2 & l1 regularization)\n",
        "        gradients = tape.gradient(total_loss, model.trainable_variables)\n",
        "\n",
        "        ## use the optimizor to update the gradient\n",
        "        opt.apply_gradients(\n",
        "            zip(gradients, model.trainable_variables))  ## based on the gradient to upgrade the model weights\n",
        "        train_loss(loss)  ## loss is calculated from binary crossentropy\n",
        "        train_auROC(y_true=train_label, y_pred=output_probs)\n",
        "\n",
        "\n",
        "\n",
        "# @tf.function(input_signature=train_step_signature)\n",
        "def valid_step(valid_seq, valid_label):\n",
        "    inf_probs, _ = model(valid_seq, training=False)\n",
        "    # print(\"Val Gated Attention:\")\n",
        "    # tf.print(_ ,output_stream=sys.stdout)\n",
        "    vloss = tf.keras.losses.BinaryCrossentropy()(y_true=valid_label, y_pred=inf_probs)\n",
        "    valid_loss(vloss)\n",
        "    valid_auROC(y_true=valid_label, y_pred=inf_probs)\n",
        "\n",
        "\n",
        "EPOCHS = 100\n",
        "current_monitor = np.inf\n",
        "patient_count = 0\n",
        "\n",
        "\n",
        "for epoch in tf.range(1, EPOCHS + 1):\n",
        "    train_loss.reset_states()\n",
        "    valid_loss.reset_states()\n",
        "\n",
        "    train_auROC.reset_states()\n",
        "    valid_auROC.reset_states()\n",
        "\n",
        "    epoch_start_time = time.time()\n",
        "    for tdata in train_dataset:\n",
        "        train_step(tdata[0], tdata[1])  ## train the dataset,tdata[0] stands for sequences (bags), tdata[1]:label\n",
        "        sys.exit()\n",
        "    print('Training of epoch {} finished! Time cost is {}s'.format(epoch, round(time.time() - epoch_start_time, 2)))\n",
        "\n",
        "    valid_start_time = time.time()\n",
        "    for vdata in valid_dataset:\n",
        "        valid_step(vdata[0], vdata[1])  ## train the validation dataset\n",
        "\n",
        "    new_valid_monitor = np.round(valid_loss.result().numpy(), 4)\n",
        "    if new_valid_monitor < current_monitor:\n",
        "        print('val_loss improved from {} to {}'.\n",
        "              format(str(current_monitor), str(new_valid_monitor)))\n",
        "        current_monitor = new_valid_monitor\n",
        "        patient_count = 0\n",
        "    else:\n",
        "        print('val_loss did not improved from {}'.format(str(current_monitor)))\n",
        "        patient_count += 1\n",
        "\n",
        "    if patient_count == 5:\n",
        "        break\n",
        "\n",
        "    template = \"Epoch {}, Time Cost: {}s, TL: {}, TROC: {}, VL:{}, VROC: {}\"\n",
        "    print(template.format(epoch, str(round(time.time() - valid_start_time, 2)),\n",
        "                          str(np.round(train_loss.result().numpy(), 4)),\n",
        "                          str(np.round(train_auROC.result().numpy(), 4)),\n",
        "                          str(np.round(valid_loss.result().numpy(), 4)),\n",
        "                          str(np.round(valid_auROC.result().numpy(), 4)),\n",
        "                          )\n",
        "          )\n"
      ],
      "metadata": {
        "id": "al-yM8kNXvUx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "091e79d3-bade-4831-e45a-ba6476f45b3d"
      },
      "id": "al-yM8kNXvUx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-eac6b8425792>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## train the dataset,tdata[0] stands for sequences (bags), tdata[1]:label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training of epoch {} finished! Time cost is {}s'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepoch_start_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-eac6b8425792>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(train_seq, train_label)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0moutput_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m# print(\"Train Gated Attenion:\",_)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# print(_.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WeakRM"
      ],
      "metadata": {
        "id": "EXy72GZ7aaff"
      },
      "id": "EXy72GZ7aaff"
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/daiyun02211/WeakRM.git /content/drive/MyDrive/weakRM_Demo\n",
        "%cd /content/drive/MyDrive/weakRM_Demo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNnP8e6AXzeE",
        "outputId": "086e46a9-1b55-4f22-8ef1-4548ebc5d5a5"
      },
      "id": "DNnP8e6AXzeE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/weakRM_Demo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python ./Scripts/token2npy.py --input_dir='./Data/m7G/' --output_dir='./Data/m7G/processed/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0n9eCwznaojq",
        "outputId": "871e6dfc-bcf7-46e8-93bd-6decc8ab5a57"
      },
      "id": "0n9eCwznaojq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"./Scripts/token2npy.py\", line 3, in <module>\n",
            "    from utils import create_folder, embed\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 818, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 917, in get_data\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python ./Scripts/main.py --training=True --input_dir='./Data/m7G/processed/' --model_name=WSCNN --fusion_method=MAX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zh1JF4x3cQLf",
        "outputId": "df47f4d3-d7c1-428f-8aa1-330d405217a7"
      },
      "id": "zh1JF4x3cQLf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data ...\n",
            "WARNING:tensorflow:From /content/drive/MyDrive/weakRM_Demo/Scripts/training.py:35: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use output_signature instead\n",
            "WARNING:tensorflow:From /content/drive/MyDrive/weakRM_Demo/Scripts/training.py:35: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_shapes is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use output_signature instead\n",
            "2022-05-05 21:53:37.664362: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "Creating model ...\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Training of epoch 1 finished! Time cost is 35.13s\n",
            "val_loss improved from inf to 0.7123, saving closed\n",
            "Epoch 1, Time Cost: 1.45s, TL: 0.7058, TROC: 0.5255, VL:0.7123, VROC: 0.6392\n",
            "Training of epoch 2 finished! Time cost is 16.81s\n",
            "val_loss improved from 0.7123 to 0.6799, saving closed\n",
            "Epoch 2, Time Cost: 1.07s, TL: 0.6823, TROC: 0.5874, VL:0.6799, VROC: 0.6478\n",
            "Training of epoch 3 finished! Time cost is 16.56s\n",
            "val_loss improved from 0.6799 to 0.6699, saving closed\n",
            "Epoch 3, Time Cost: 1.04s, TL: 0.6684, TROC: 0.6492, VL:0.6699, VROC: 0.6512\n",
            "Training of epoch 4 finished! Time cost is 16.58s\n",
            "val_loss improved from 0.6699 to 0.6634, saving closed\n",
            "Epoch 4, Time Cost: 1.04s, TL: 0.6562, TROC: 0.68, VL:0.6634, VROC: 0.6578\n",
            "Training of epoch 5 finished! Time cost is 16.69s\n",
            "val_loss improved from 0.6634 to 0.657, saving closed\n",
            "Epoch 5, Time Cost: 1.05s, TL: 0.649, TROC: 0.6917, VL:0.657, VROC: 0.6593\n",
            "Training of epoch 6 finished! Time cost is 16.55s\n",
            "val_loss improved from 0.657 to 0.6542, saving closed\n",
            "Epoch 6, Time Cost: 1.09s, TL: 0.6449, TROC: 0.6998, VL:0.6542, VROC: 0.6653\n",
            "Training of epoch 7 finished! Time cost is 16.56s\n",
            "val_loss improved from 0.6542 to 0.6502, saving closed\n",
            "Epoch 7, Time Cost: 1.1s, TL: 0.6401, TROC: 0.713, VL:0.6502, VROC: 0.6812\n",
            "Training of epoch 8 finished! Time cost is 16.66s\n",
            "val_loss improved from 0.6502 to 0.647, saving closed\n",
            "Epoch 8, Time Cost: 1.02s, TL: 0.6348, TROC: 0.7264, VL:0.647, VROC: 0.6928\n",
            "Training of epoch 9 finished! Time cost is 16.56s\n",
            "val_loss improved from 0.647 to 0.6413, saving closed\n",
            "Epoch 9, Time Cost: 1.06s, TL: 0.6301, TROC: 0.739, VL:0.6413, VROC: 0.7114\n",
            "Training of epoch 10 finished! Time cost is 16.5s\n",
            "val_loss improved from 0.6413 to 0.635, saving closed\n",
            "Epoch 10, Time Cost: 1.08s, TL: 0.6204, TROC: 0.7568, VL:0.635, VROC: 0.7308\n",
            "Training of epoch 11 finished! Time cost is 16.61s\n",
            "val_loss improved from 0.635 to 0.6305, saving closed\n",
            "Epoch 11, Time Cost: 1.09s, TL: 0.6144, TROC: 0.7721, VL:0.6305, VROC: 0.743\n",
            "Training of epoch 12 finished! Time cost is 16.53s\n",
            "val_loss improved from 0.6305 to 0.6245, saving closed\n",
            "Epoch 12, Time Cost: 1.06s, TL: 0.6108, TROC: 0.7773, VL:0.6245, VROC: 0.7616\n",
            "Training of epoch 13 finished! Time cost is 16.64s\n",
            "val_loss improved from 0.6245 to 0.6173, saving closed\n",
            "Epoch 13, Time Cost: 1.07s, TL: 0.6034, TROC: 0.7927, VL:0.6173, VROC: 0.7757\n",
            "Training of epoch 14 finished! Time cost is 16.6s\n",
            "val_loss improved from 0.6173 to 0.611, saving closed\n",
            "Epoch 14, Time Cost: 1.05s, TL: 0.5965, TROC: 0.7979, VL:0.611, VROC: 0.7926\n",
            "Training of epoch 15 finished! Time cost is 16.64s\n",
            "val_loss improved from 0.611 to 0.6047, saving closed\n",
            "Epoch 15, Time Cost: 1.06s, TL: 0.5892, TROC: 0.8051, VL:0.6047, VROC: 0.8002\n",
            "Training of epoch 16 finished! Time cost is 16.52s\n",
            "val_loss improved from 0.6047 to 0.6013, saving closed\n",
            "Epoch 16, Time Cost: 1.28s, TL: 0.5839, TROC: 0.8072, VL:0.6013, VROC: 0.8087\n",
            "Training of epoch 17 finished! Time cost is 16.67s\n",
            "val_loss improved from 0.6013 to 0.5963, saving closed\n",
            "Epoch 17, Time Cost: 1.06s, TL: 0.5791, TROC: 0.8107, VL:0.5963, VROC: 0.8105\n",
            "Training of epoch 18 finished! Time cost is 16.62s\n",
            "val_loss improved from 0.5963 to 0.5936, saving closed\n",
            "Epoch 18, Time Cost: 1.05s, TL: 0.5792, TROC: 0.8039, VL:0.5936, VROC: 0.8086\n",
            "Training of epoch 19 finished! Time cost is 17.01s\n",
            "val_loss improved from 0.5936 to 0.5887, saving closed\n",
            "Epoch 19, Time Cost: 1.07s, TL: 0.5728, TROC: 0.8111, VL:0.5887, VROC: 0.8156\n",
            "Training of epoch 20 finished! Time cost is 16.76s\n",
            "val_loss improved from 0.5887 to 0.5863, saving closed\n",
            "Epoch 20, Time Cost: 1.05s, TL: 0.5704, TROC: 0.8095, VL:0.5863, VROC: 0.8127\n",
            "Test AUC:  0.8058229024982351\n",
            "Test PRC:  0.7791907104641502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "frQjbZKrcdel"
      },
      "id": "frQjbZKrcdel",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "main_fanfan.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "byHmNhx0VrK5"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
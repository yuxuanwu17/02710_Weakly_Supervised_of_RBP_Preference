{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "722178ed-e1c5-4348-ac97-144cf339b538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.utils as utils\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "from torchsummaryX import summary\n",
    "import pytorch_warmup as warmup\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from dataloader import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33c1f01f-2acb-463a-aaf7-1fc4cc489a85",
   "metadata": {
    "id": "33c1f01f-2acb-463a-aaf7-1fc4cc489a85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 40, 4]) torch.Size([1, 2]) torch.Size([1, 13, 40, 6])\n"
     ]
    }
   ],
   "source": [
    "path = '../iDeepS/datasets/clip/11_CLIPSEQ_ELAVL1_hg19'\n",
    "\n",
    "instance_length = 40\n",
    "instance_stride = 5\n",
    "\n",
    "batch_size = 1\n",
    "epochs = 10\n",
    "lr = 5e-4\n",
    "weight_decay = 1e-5\n",
    "\n",
    "train_data_path = path + \"/30000/training_sample_0/sequences.fa.gz\"\n",
    "valid_data_path = path + \"/30000/test_sample_0/sequences.fa.gz\"\n",
    "train_structure_path = path + \"/30000/training_sample_0/sequence_structures_forgi.out\"\n",
    "validate_structure_path = path + \"/30000/test_sample_0/sequence_structures_forgi.out\"\n",
    "\n",
    "train_data = LibriSamplesWithStructure(train_data_path, train_structure_path)\n",
    "valid_data = LibriSamplesWithStructure(valid_data_path, validate_structure_path)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "for x, y, structure in train_loader:\n",
    "    print(x.shape, y.shape, structure.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1896e531-6711-49dc-84ac-24b333a2beea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeakRMwithStructure(nn.Module):\n",
    "    \"\"\"\n",
    "    used for channel = 4, AGCT\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, training=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.inst_conv = nn.Sequential(\n",
    "            nn.Conv1d(4, 32, kernel_size=15, padding=7, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv1d(32, 16, kernel_size=5, padding=2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self.structure_conv = nn.Sequential(\n",
    "            nn.Conv1d(6, 32, kernel_size=15, padding=7, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv1d(32, 16, kernel_size=5, padding=2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self.attention_v = nn.Sequential(\n",
    "            nn.Linear(640, 128),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.attention_u = nn.Sequential(\n",
    "            nn.Linear(640, 128),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.attention_weights = nn.Sequential(\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(640, 2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, inputs, structures, training=True, mask=None):\n",
    "        inputs = torch.squeeze(inputs, 0)\n",
    "        inputs = inputs.permute((0, 2, 1)) \n",
    "        \n",
    "        structures = torch.squeeze(structures, 0)\n",
    "        structures = structures.permute((0, 2, 1)) \n",
    "        # print(structures.shape)\n",
    "        inst_features = self.inst_conv(inputs) \n",
    "        structure_features = self.structure_conv(structures)\n",
    "        \n",
    "        # print(inst_features.shape, structure_features.shape)\n",
    "        inst_features = torch.cat((inst_features, structure_features), dim = 1)\n",
    "        # print(inst_features.shape)\n",
    "        attention_v = self.attention_v(inst_features)\n",
    "        attention_u = self.attention_v(inst_features)\n",
    "\n",
    "        # print(attention_u*attention_v)\n",
    "        # print(self.attention_weights(attention_u * attention_v))\n",
    "\n",
    "        gated_attention = self.attention_weights(attention_u * attention_v).permute((1, 0))\n",
    "        # print(gated_attention)\n",
    "\n",
    "        gated_attention = self.softmax(gated_attention)  # torch.Size([1, 13])\n",
    "\n",
    "        bag_features = torch.matmul(gated_attention, inst_features)\n",
    "\n",
    "        bag_probability = self.cls(bag_features)\n",
    "\n",
    "        return bag_probability, gated_attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ZwbrkD9MtjyT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZwbrkD9MtjyT",
    "outputId": "85591642-c170-4a5b-cc85-7f815fbe072c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2270515/1773639120.py:71: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gated_attention = self.softmax(gated_attention)  # torch.Size([1, 13])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy 84.9400%, Auc Score 0.9035%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2270515/1773639120.py:71: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gated_attention = self.softmax(gated_attention)  # torch.Size([1, 13])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy 84.1700%, Auc Score 0.9002%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2270515/1773639120.py:71: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gated_attention = self.softmax(gated_attention)  # torch.Size([1, 13])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy 87.9900%, Auc Score 0.9052%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2270515/1773639120.py:71: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gated_attention = self.softmax(gated_attention)  # torch.Size([1, 13])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy 84.8900%, Auc Score 0.9030%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2270515/1773639120.py:71: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gated_attention = self.softmax(gated_attention)  # torch.Size([1, 13])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy 86.8700%, Auc Score 0.9049%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2270515/1773639120.py:71: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gated_attention = self.softmax(gated_attention)  # torch.Size([1, 13])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2270515/1616093249.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mstructure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# print(structure[:, :13, :, :].shape, structure.shape, \"!\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2270515/1773639120.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, structures, training, mask)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mstructures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# print(structures.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0minst_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minst_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mstructure_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructure_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    296\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[0;32m--> 298\u001b[0;31m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    299\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = WeakRMwithStructure().cuda()\n",
    "# model = WSCNN().cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.005)\n",
    "criterion = nn.BCELoss(weight=torch.tensor([0.8, 0.2])).cuda()\n",
    "num_steps = len(train_loader) * epochs\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_steps)\n",
    "warmup_scheduler = warmup.RAdamWarmup(optimizer)\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "        # training\n",
    "        model.train()\n",
    "        num_correct = 0\n",
    "        total_loss = 0\n",
    "        for i, (x, y, structure) in enumerate(train_loader):\n",
    "            # print(i)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x = x.float().cuda()\n",
    "            y = y.float().cuda()\n",
    "            structure = structure.float().cuda()\n",
    "            # print(structure[:, :13, :, :].shape, structure.shape, \"!\")\n",
    "            outputs, _ = model(x, structure[:, :13, :, :])\n",
    "\n",
    "            loss = criterion(outputs[0], y[0])\n",
    "\n",
    "            outputs = torch.argmax(outputs)\n",
    "\n",
    "            num_correct += int((outputs == torch.argmax(y)).sum())\n",
    "            total_loss += loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with warmup_scheduler.dampening():\n",
    "                scheduler.step()\n",
    "\n",
    "        print_content =\"Epoch {}/{}: Train Acc {:.04f}%, Train Loss {:.04f}, Learning Rate {:.04f}\".format(\n",
    "            epoch,\n",
    "            epochs,\n",
    "            100 * num_correct / (len(train_loader) * batch_size),\n",
    "            float(total_loss / len(train_loader)),\n",
    "            float(optimizer.param_groups[0]['lr'])\n",
    "        )\n",
    "\n",
    "#         print(print_content)\n",
    "        train_acc =  100 * num_correct / (len(train_loader) * batch_size)\n",
    "        train_loss = float(total_loss / len(train_loader))\n",
    "        learn_rate = float(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        # eval\n",
    "        model.eval()\n",
    "\n",
    "        num_correct = 0\n",
    "        total_loss = 0\n",
    "        predictions =[]\n",
    "        labels = []\n",
    "        for i, (x, y, structure) in enumerate(valid_loader):\n",
    "            x = x.float().cuda()\n",
    "            y = y.float().cuda()\n",
    "            structure = structure.float().cuda()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs_probs, _ = model(x, structure[:, :13, :, :])\n",
    "\n",
    "            outputs = torch.argmax(outputs_probs)\n",
    "\n",
    "            num_correct += int((outputs == torch.argmax(y)).sum())\n",
    "            total_loss += loss\n",
    "\n",
    "            predictions.append(outputs_probs.detach().cpu().numpy()[0])\n",
    "            labels.append(torch.argmax(y).detach().cpu().numpy())\n",
    "\n",
    "        dev_acc = 100 * num_correct / len(valid_loader)\n",
    "        dev_loss = total_loss / len(valid_loader)\n",
    "\n",
    "        auc_score = roc_auc_score(np.array(labels).flatten(), np.array(predictions)[:, 1])\n",
    "        print(\"Validation Accuracy {:.04f}%, Auc Score {:.04f}%\".format(dev_acc, auc_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbf1a76-dbe2-44fe-a2d1-6a6dc320ed83",
   "metadata": {},
   "outputs": [],
   "source": [
    "[1, 2, 3, 4][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "570b4841-99c0-43c0-b8d8-111872b763f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'structures' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2270515/629526606.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstructures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'structures' is not defined"
     ]
    }
   ],
   "source": [
    "structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3543bbf6-a00c-441c-8bdf-61ef9ee7e293",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main_fanfan.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "722178ed-e1c5-4348-ac97-144cf339b538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.utils as utils\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "from torchsummaryX import summary\n",
    "import pytorch_warmup as warmup\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import wandb\n",
    "\n",
    "from dataloader import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf737abd-a739-4774-9103-41613d0a7dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    " '../iDeepS/datasets/clip/10_PARCLIP_ELAVL1A_hg19',\n",
    " # '../iDeepS/datasets/clip/11_CLIPSEQ_ELAVL1_hg19',\n",
    " # '../iDeepS/datasets/clip/12_PARCLIP_EWSR1_hg19',\n",
    " # '../iDeepS/datasets/clip/13_PARCLIP_FUS_hg19',\n",
    " # '../iDeepS/datasets/clip/14_PARCLIP_FUS_mut_hg19',\n",
    " # '../iDeepS/datasets/clip/15_PARCLIP_IGF2BP123_hg19',\n",
    " # '../iDeepS/datasets/clip/16_ICLIP_hnRNPC_Hela_iCLIP_all_clusters',\n",
    " # '../iDeepS/datasets/clip/18_ICLIP_hnRNPL_Hela_group_3975_all-hnRNPL-Hela-hg19_sum_G_hg19--ensembl59_from_2337-2339-741_bedGraph-cDNA-hits-in-genome',\n",
    " # '../iDeepS/datasets/clip/19_ICLIP_hnRNPL_U266_group_3986_all-hnRNPL-U266-hg19_sum_G_hg19--ensembl59_from_2485_bedGraph-cDNA-hits-in-genome',\n",
    " # '../iDeepS/datasets/clip/17_ICLIP_HNRNPC_hg19',\n",
    " # '../iDeepS/datasets/clip/1_PARCLIP_AGO1234_hg19',\n",
    " # '../iDeepS/datasets/clip/20_ICLIP_hnRNPlike_U266_group_4000_all-hnRNPLlike-U266-hg19_sum_G_hg19--ensembl59_from_2342-2486_bedGraph-cDNA-hits-in-genome',\n",
    " # '../iDeepS/datasets/clip/21_PARCLIP_MOV10_Sievers_hg19',\n",
    " # '../iDeepS/datasets/clip/22_ICLIP_NSUN2_293_group_4007_all-NSUN2-293-hg19_sum_G_hg19--ensembl59_from_3137-3202_bedGraph-cDNA-hits-in-genome',\n",
    " # '../iDeepS/datasets/clip/23_PARCLIP_PUM2_hg19',\n",
    " # '../iDeepS/datasets/clip/24_PARCLIP_QKI_hg19',\n",
    " # '../iDeepS/datasets/clip/25_CLIPSEQ_SFRS1_hg19',\n",
    " '../iDeepS/datasets/clip/26_PARCLIP_TAF15_hg19',\n",
    " # '../iDeepS/datasets/clip/27_ICLIP_TDP43_hg19',\n",
    " # '../iDeepS/datasets/clip/28_ICLIP_TIA1_hg19',\n",
    " # '../iDeepS/datasets/clip/29_ICLIP_TIAL1_hg19',\n",
    " # '../iDeepS/datasets/clip/2_PARCLIP_AGO2MNASE_hg19',\n",
    " # '../iDeepS/datasets/clip/30_ICLIP_U2AF65_Hela_iCLIP_ctrl_all_clusters',\n",
    " # '../iDeepS/datasets/clip/31_ICLIP_U2AF65_Hela_iCLIP_ctrl+kd_all_clusters',\n",
    " # '../iDeepS/datasets/clip/3_HITSCLIP_Ago2_binding_clusters',\n",
    " # '../iDeepS/datasets/clip/4_HITSCLIP_Ago2_binding_clusters_2',\n",
    " # '../iDeepS/datasets/clip/5_CLIPSEQ_AGO2_hg19',\n",
    " # '../iDeepS/datasets/clip/6_CLIP-seq-eIF4AIII_1',\n",
    " # '../iDeepS/datasets/clip/7_CLIP-seq-eIF4AIII_2',\n",
    " # '../iDeepS/datasets/clip/8_PARCLIP_ELAVL1_hg19',\n",
    " # '../iDeepS/datasets/clip/9_PARCLIP_ELAVL1MNASE_hg19'\n",
    "]\n",
    "\n",
    "instance_length = 40\n",
    "instance_stride = 5\n",
    "\n",
    "batch_size = 1\n",
    "epochs = 10\n",
    "lr = 5e-4\n",
    "weight_decay = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb3d1db5-d9c9-44ee-a21d-b27e5e29c170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(path):\n",
    "    print(f\"start running {path}\")\n",
    "    \n",
    "    wandb.init(project=\"02710\", entity=\"fanfanwu9898\",name = \"WEAKRMLSTM\" + p.split(\"/\")[-1].split(\"_\")[0])\n",
    "    wandb.config = {\n",
    "      \"learning_rate\": lr,\n",
    "      \"epochs\": epochs,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"instance_length\" : instance_length,\n",
    "      \"instance_stride\" : instance_stride,\n",
    "      \"decay\": weight_decay\n",
    "    }\n",
    "    \n",
    "\n",
    "    train_data_path = path + \"/30000/training_sample_0/sequences.fa.gz\"\n",
    "    valid_data_path = path + \"/30000/test_sample_0/sequences.fa.gz\"\n",
    "    train_data = LibriSamples(train_data_path)\n",
    "    valid_data = LibriSamples(valid_data_path)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_data, batch_size = batch_size, shuffle=True)\n",
    "    \n",
    "    model = WeakRMLSTM().cuda()\n",
    "    # model = WSCNN().cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.BCELoss(weight=torch.tensor([0.8, 0.2])).cuda()\n",
    "    num_steps = len(train_loader) * epochs\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_steps)\n",
    "    warmup_scheduler = warmup.RAdamWarmup(optimizer)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # training\n",
    "        model.train()\n",
    "        num_correct = 0\n",
    "        total_loss = 0\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x = x.float().cuda()\n",
    "            y = y.float().cuda()\n",
    "\n",
    "            outputs, _ = model(x)\n",
    "\n",
    "            loss = criterion(outputs[0], y[0])\n",
    "\n",
    "            outputs = torch.argmax(outputs)\n",
    "\n",
    "            num_correct += int((outputs == torch.argmax(y)).sum())\n",
    "            total_loss += loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with warmup_scheduler.dampening():\n",
    "                scheduler.step()\n",
    "\n",
    "#         print_content =\"Epoch {}/{}: Train Acc {:.04f}%, Train Loss {:.04f}, Learning Rate {:.04f}\".format(\n",
    "#             epoch,\n",
    "#             epochs,\n",
    "#             100 * num_correct / (len(train_loader) * batch_size),\n",
    "#             float(total_loss / len(train_loader)),\n",
    "#             float(optimizer.param_groups[0]['lr'])\n",
    "#         )\n",
    "\n",
    "#         print(print_content)\n",
    "        train_acc =  100 * num_correct / (len(train_loader) * batch_size)\n",
    "        train_loss = float(total_loss / len(train_loader))\n",
    "        learn_rate = float(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        # eval\n",
    "        model.eval()\n",
    "\n",
    "        num_correct = 0\n",
    "        total_loss = 0\n",
    "        predictions =[]\n",
    "        labels = []\n",
    "        for i, (x, y) in enumerate(valid_loader):\n",
    "            x = x.float().cuda()\n",
    "            y = y.float().cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs_probs,_ = model(x)\n",
    "\n",
    "            outputs = torch.argmax(outputs_probs)\n",
    "\n",
    "            num_correct += int((outputs == torch.argmax(y)).sum())\n",
    "            total_loss += loss\n",
    "\n",
    "            predictions.append(outputs_probs.detach().cpu().numpy()[0])\n",
    "            labels.append(torch.argmax(y).detach().cpu().numpy())\n",
    "\n",
    "        dev_acc = 100 * num_correct / len(valid_loader)\n",
    "        dev_loss = total_loss / len(valid_loader)\n",
    "\n",
    "        auc_score = roc_auc_score(np.array(labels).flatten(), np.array(predictions)[:, 1])\n",
    "        # print(\"Validation Accuracy {:.04f}%, Auc Score {:.04f}%\".format(dev_acc, auc_score))\n",
    "        \n",
    "        wandb.log({\"Train Acc:\":train_acc, \"Train loss:\": train_loss, \"Test Acc \":dev_acc,\n",
    "                   \"Test loss\":dev_loss, \"auROC\":auc_score, \"lr\":lr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f27d8c5d-923d-43ba-8e8d-e05c7cae986b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start running ../iDeepS/datasets/clip/10_PARCLIP_ELAVL1A_hg19\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2p5hzfxw) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Acc </td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Test loss</td><td>▂██▂▂▁▁▁▁▁</td></tr><tr><td>Train Acc:</td><td>▁█████████</td></tr><tr><td>Train loss:</td><td>█▆▆▅▄▃▃▂▁▁</td></tr><tr><td>auROC</td><td>▁▂▄▄▇▇████</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Acc </td><td>80.0</td></tr><tr><td>Test loss</td><td>0.10578</td></tr><tr><td>Train Acc:</td><td>80.0</td></tr><tr><td>Train loss:</td><td>0.24374</td></tr><tr><td>auROC</td><td>0.58706</td></tr><tr><td>lr</td><td>0.0005</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">WEAKRMLSTM9</strong>: <a href=\"https://wandb.ai/fanfanwu9898/02710/runs/2p5hzfxw\" target=\"_blank\">https://wandb.ai/fanfanwu9898/02710/runs/2p5hzfxw</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220509_103141-2p5hzfxw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2p5hzfxw). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/yifan/Desktop/genomics/projects/model/wandb/run-20220509_151731-3vyo0zfi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fanfanwu9898/02710/runs/3vyo0zfi\" target=\"_blank\">WEAKRMLSTM10</a></strong> to <a href=\"https://wandb.ai/fanfanwu9898/02710\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yifan/Desktop/genomics/projects/model/model.py:137: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gated_attention = nn.Softmax()(gated_attention)  # torch.Size([1, 13])\n",
      "/home/yifan/Desktop/genomics/projects/model/model.py:137: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gated_attention = nn.Softmax()(gated_attention)  # torch.Size([1, 13])\n",
      "/home/yifan/Desktop/genomics/projects/model/model.py:137: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gated_attention = nn.Softmax()(gated_attention)  # torch.Size([1, 13])\n",
      "/home/yifan/Desktop/genomics/projects/model/model.py:137: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gated_attention = nn.Softmax()(gated_attention)  # torch.Size([1, 13])\n",
      "/home/yifan/Desktop/genomics/projects/model/model.py:137: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gated_attention = nn.Softmax()(gated_attention)  # torch.Size([1, 13])\n",
      "/home/yifan/Desktop/genomics/projects/model/model.py:137: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gated_attention = nn.Softmax()(gated_attention)  # torch.Size([1, 13])\n",
      "/home/yifan/Desktop/genomics/projects/model/model.py:137: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gated_attention = nn.Softmax()(gated_attention)  # torch.Size([1, 13])\n",
      "/home/yifan/Desktop/genomics/projects/model/model.py:137: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gated_attention = nn.Softmax()(gated_attention)  # torch.Size([1, 13])\n",
      "/home/yifan/Desktop/genomics/projects/model/model.py:137: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gated_attention = nn.Softmax()(gated_attention)  # torch.Size([1, 13])\n",
      "/home/yifan/Desktop/genomics/projects/model/model.py:137: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gated_attention = nn.Softmax()(gated_attention)  # torch.Size([1, 13])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start running ../iDeepS/datasets/clip/26_PARCLIP_TAF15_hg19\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3vyo0zfi) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Acc </td><td>▅▇▇██▅▂▃▂▁</td></tr><tr><td>Test loss</td><td>▂▁█▁▂▁▁▁▁▂</td></tr><tr><td>Train Acc:</td><td>▁▃▄▄▅▆▆▇██</td></tr><tr><td>Train loss:</td><td>█▆▆▅▅▄▃▂▁▁</td></tr><tr><td>auROC</td><td>▇████▆▃▃▁▁</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Acc </td><td>88.06</td></tr><tr><td>Test loss</td><td>0.05529</td></tr><tr><td>Train Acc:</td><td>92.24333</td></tr><tr><td>Train loss:</td><td>0.10563</td></tr><tr><td>auROC</td><td>0.87683</td></tr><tr><td>lr</td><td>0.0005</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">WEAKRMLSTM10</strong>: <a href=\"https://wandb.ai/fanfanwu9898/02710/runs/3vyo0zfi\" target=\"_blank\">https://wandb.ai/fanfanwu9898/02710/runs/3vyo0zfi</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220509_151731-3vyo0zfi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3vyo0zfi). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/yifan/Desktop/genomics/projects/model/wandb/run-20220509_153618-38vudbkt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fanfanwu9898/02710/runs/38vudbkt\" target=\"_blank\">WEAKRMLSTM26</a></strong> to <a href=\"https://wandb.ai/fanfanwu9898/02710\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yifan/Desktop/genomics/projects/model/model.py:137: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gated_attention = nn.Softmax()(gated_attention)  # torch.Size([1, 13])\n",
      "/home/yifan/Desktop/genomics/projects/model/model.py:137: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gated_attention = nn.Softmax()(gated_attention)  # torch.Size([1, 13])\n",
      "/home/yifan/Desktop/genomics/projects/model/model.py:137: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gated_attention = nn.Softmax()(gated_attention)  # torch.Size([1, 13])\n",
      "/home/yifan/Desktop/genomics/projects/model/model.py:137: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gated_attention = nn.Softmax()(gated_attention)  # torch.Size([1, 13])\n",
      "/home/yifan/Desktop/genomics/projects/model/model.py:137: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gated_attention = nn.Softmax()(gated_attention)  # torch.Size([1, 13])\n",
      "/home/yifan/Desktop/genomics/projects/model/model.py:137: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gated_attention = nn.Softmax()(gated_attention)  # torch.Size([1, 13])\n",
      "/home/yifan/Desktop/genomics/projects/model/model.py:137: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gated_attention = nn.Softmax()(gated_attention)  # torch.Size([1, 13])\n",
      "/home/yifan/Desktop/genomics/projects/model/model.py:137: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gated_attention = nn.Softmax()(gated_attention)  # torch.Size([1, 13])\n",
      "/home/yifan/Desktop/genomics/projects/model/model.py:137: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gated_attention = nn.Softmax()(gated_attention)  # torch.Size([1, 13])\n",
      "/home/yifan/Desktop/genomics/projects/model/model.py:137: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  gated_attention = nn.Softmax()(gated_attention)  # torch.Size([1, 13])\n"
     ]
    }
   ],
   "source": [
    "for p in paths:\n",
    "    train(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7988286c-2a13-4301-b9a6-359d2638f27f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main_fanfan.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
